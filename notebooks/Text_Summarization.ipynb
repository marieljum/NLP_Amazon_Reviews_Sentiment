{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf79bba4-138d-4937-810a-99ac2354ed4c",
   "metadata": {},
   "source": [
    "# Text Summarization of Amazon Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c66c1d2d-5190-4a5b-a4dd-08e620fd81f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import seaborn as sns\n",
    "import random\n",
    "import html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import textacy\n",
    "\n",
    "# Text cleaning\n",
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Cosine Similarity \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Text summarization \n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "import textdistance\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad46dc4d-99a0-42a2-86e1-2be2a26e242b",
   "metadata": {},
   "source": [
    "## Load saved reviews and metadata csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b6b1d06e-3eae-42a0-9cba-3580294269e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rating', 'title', 'text', 'asin', 'parent_asin', 'user_id',\n",
       "       'timestamp', 'helpful_vote', 'verified_purchase', 'year',\n",
       "       'main_category', 'average_rating', 'rating_number', 'features',\n",
       "       'description', 'price', 'images', 'videos', 'store', 'categories',\n",
       "       'details', 'bought_together', 'subtitle', 'author'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df = pd.read_csv('../data/merge_df.csv', low_memory = False, index_col = False)\n",
    "merge_df = merge_df.drop(columns = {'Unnamed: 0'})\n",
    "merge_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "502decd7-2ad8-45a4-8016-3c4451bbf0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null values in 'text' column \n",
    "merge_df = merge_df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0555b2f7-f431-4a6d-9ab1-612f65d9491b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique store count: 245290\n",
      "-----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "store\n",
       "Coleman                   11886\n",
       "Fitbit                     6787\n",
       "CamelBak                   4897\n",
       "Alvada                     4339\n",
       "Franklin Sports            4142\n",
       "BalanceFrom                3563\n",
       "Amazon Basics              3516\n",
       "WILSON                     3178\n",
       "WinCraft                   3094\n",
       "CAP Barbell                3026\n",
       "Outdoorsman Lab            2971\n",
       "FOCO                       2910\n",
       "Schwinn                    2877\n",
       "Contigo                    2867\n",
       "Sunny Health & Fitness     2794\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Unique store count: {merge_df['parent_asin'].nunique()}\")\n",
    "print('-----------')\n",
    "merge_df['store'].value_counts().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0eb12257-131e-4557-a6a8-0b7704dd1ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "store\n",
       "Coleman                   11886\n",
       "Fitbit                     6787\n",
       "CamelBak                   4897\n",
       "Alvada                     4339\n",
       "Franklin Sports            4142\n",
       "BalanceFrom                3563\n",
       "Amazon Basics              3516\n",
       "WILSON                     3178\n",
       "WinCraft                   3094\n",
       "CAP Barbell                3026\n",
       "Outdoorsman Lab            2971\n",
       "FOCO                       2910\n",
       "Schwinn                    2877\n",
       "Contigo                    2867\n",
       "Sunny Health & Fitness     2794\n",
       "SHIMANO                    2727\n",
       "adidas                     2611\n",
       "Gaiam                      2563\n",
       "Rico Industries            2452\n",
       "Nalgene                    2407\n",
       "Speedo                     2337\n",
       "Razor                      2304\n",
       "Sportneer                  2146\n",
       "Yes4All                    2103\n",
       "Northwest                  2095\n",
       "BELL                       2075\n",
       "New Era                    1968\n",
       "OPAMOO                     1905\n",
       "Siskiyou Sports            1872\n",
       "Intex                      1835\n",
       "KastKing                   1824\n",
       "Under Armour               1820\n",
       "SRAM                       1781\n",
       "BV                         1779\n",
       "Callaway                   1739\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df['store'].value_counts().head(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47834b08-0725-4661-bf11-44688225b575",
   "metadata": {},
   "source": [
    "## Filtering the dataframe to a specific brand or product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b16e2df2-d98d-4cef-a050-5f2e9e8ff4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rating', 'title', 'text', 'asin', 'parent_asin', 'user_id',\n",
       "       'timestamp', 'helpful_vote', 'verified_purchase', 'year',\n",
       "       'main_category', 'average_rating', 'rating_number', 'features',\n",
       "       'description', 'price', 'images', 'videos', 'store', 'categories',\n",
       "       'details', 'bought_together', 'subtitle', 'author'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning a specific store name\n",
    "store = 'Contigo'\n",
    "\n",
    "filtered_df = merge_df.loc[merge_df['store'] == store].copy()\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "filtered_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9d35eabd-284a-41ca-877a-465e0a97f3ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store: Contigo\n",
      "-----\n",
      "Number of reviews in complete Amazon dataset: 192942\n",
      "Number of reviews in df: 2867\n",
      "Count of unique parent product IDs: 175\n",
      "Count of unique product ID's: 293\n",
      "-----\n",
      "Top ten products: \n",
      "     parent_asin  average_rating  rating_number  composite_score\n",
      "158  B08XXQSHH6             4.8        30463.0         146222.4\n",
      "169  B0BTHXPZWZ             4.8        12267.0          58881.6\n",
      "173  B0C4VWCZRV             4.5        12678.0          57051.0\n",
      "165  B0BT9JQ4LJ             4.7        10820.0          50854.0\n",
      "121  B07GBFYHNM             4.6         9365.0          43079.0\n",
      "164  B0BT9G6ZJC             4.7         7715.0          36260.5\n",
      "142  B07PDHRYSB             4.7         7354.0          34563.8\n",
      "162  B09R9X84HF             4.7         6619.0          31109.3\n",
      "65   B00YYBBYN8             4.6         5090.0          23414.0\n",
      "155  B08PWGXDFM             4.8         4754.0          22819.2\n",
      "-----\n",
      "Bottom ten products: \n",
      "     parent_asin  average_rating  rating_number  composite_score\n",
      "6    B003KZKD9K             3.5            2.0              7.0\n",
      "2    B00260GLOG             3.3            9.0             29.7\n",
      "119  B0786N8T17             3.9            8.0             31.2\n",
      "14   B004CYEB7S             4.0            8.0             32.0\n",
      "157  B08TX6H9SK             5.0            8.0             40.0\n",
      "5    B003KZKD7C             3.2           16.0             51.2\n",
      "12   B0044WIEAI             4.0           13.0             52.0\n",
      "3    B00260GLOQ             3.3           16.0             52.8\n",
      "0    B001QW0YP2             4.5           12.0             54.0\n",
      "28   B00A62JED6             4.1           14.0             57.4\n"
     ]
    }
   ],
   "source": [
    "def generate_brand_report(df, store):\n",
    "    \"\"\"\n",
    "    Generates a report for a specified store, including counts of unique parent and product IDs, \n",
    "    and statistics on reviews and ratings.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame by the specified store\n",
    "    filtered_df = df[df['store'] == store]\n",
    "    print(f\"Store: {store}\")\n",
    "    print('-----')\n",
    "\n",
    "    # Count number of reviews\n",
    "    complete_reviews_count = filtered_df['rating_number'].unique().sum()\n",
    "    print(f\"Number of reviews in complete Amazon dataset: {complete_reviews_count}\")\n",
    "    print(f\"Number of reviews in df: {len(filtered_df)}\")  # Some customers only include a title and rating to their reviews\n",
    "    \n",
    "    # Calculate the number of unique parent_asin values\n",
    "    parent_asin_count = filtered_df['parent_asin'].nunique()\n",
    "    print(f\"Count of unique parent product IDs: {parent_asin_count}\")\n",
    "    \n",
    "    # Count the number of unique asin values\n",
    "    asin_count = filtered_df['asin'].nunique()\n",
    "    print(f\"Count of unique product ID's: {asin_count}\")\n",
    "    \n",
    "    # Calculate the number of reviews and average review rating for each asin\n",
    "    asin_reviews_ratings = filtered_df.groupby('parent_asin').agg({'average_rating': 'mean', 'rating_number': 'mean'}).reset_index()\n",
    "\n",
    "    # Create a composite score for each product\n",
    "    asin_reviews_ratings['composite_score'] = asin_reviews_ratings['average_rating'] * asin_reviews_ratings['rating_number']\n",
    "\n",
    "    \n",
    "    # Find the top and bottom ten products with the best and worst rating\n",
    "    top_ten_best = asin_reviews_ratings.nlargest(10, 'composite_score')\n",
    "    bottom_ten_worst = asin_reviews_ratings.nsmallest(10, 'composite_score')\n",
    "    print('-----')\n",
    "    print(f\"Top ten products: \\n {top_ten_best}\")\n",
    "    print('-----')\n",
    "    print(f\"Bottom ten products: \\n {bottom_ten_worst}\")\n",
    "\n",
    "# Apply function\n",
    "generate_brand_report(filtered_df, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "30aaff2b-fe99-43b9-8703-06f40e7b3fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_examples(df, column, interest):\n",
    "    \"\"\"\n",
    "    Generate three examples of random records with the specified interest. \n",
    "    \n",
    "    Parameters: \n",
    "    df: DataFrame where the search will occur. \n",
    "    column: Column name to search within. \n",
    "    interest: Value to search for in the specified column. \n",
    "    \"\"\"\n",
    "    # Print average rating for parent_asin \n",
    "    interest_average_rating = df[df[column] == interest]['average_rating'].unique()\n",
    "    print(f'Average rating: {interest_average_rating[0]}\\n')\n",
    "    print(f'Categories: {df[df[column] == interest][\"categories\"].iloc[0]}\\n')\n",
    "    print(f'Details: {df[df[column] == interest][\"details\"].iloc[0]}\\n')\n",
    "    \n",
    "    # Filter the Datadf[df[column] == interest]Frame based on the interest\n",
    "    filtered_df = df[df[column] == interest]\n",
    "\n",
    "    # Set number of examples \n",
    "    num_of_ex = 3 \n",
    "    \n",
    "    # Check if there are enough records to sample\n",
    "    if len(filtered_df) < num_of_ex:\n",
    "        print(f\"Not enough records found for interest '{interest}'. Found {len(filtered_df)} records.\")\n",
    "        return\n",
    "    \n",
    "    # Randomly select three records\n",
    "    random_examples = filtered_df.sample(n=num_of_ex)\n",
    "    \n",
    "    for i, row in random_examples.iterrows():\n",
    "        print('-----')\n",
    "        print(f'Rating: {row[\"rating\"]}\\n')\n",
    "        print(f'Title: {row[\"title\"]}\\n')\n",
    "        print(f'Text: {row[\"text\"]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "117e62b5-116e-4bb5-9e37-10c7366bf81f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating: 4.8\n",
      "\n",
      "Categories: ['Sports & Outdoors', 'Sports & Outdoor Recreation Accessories', 'Sports Water Bottles']\n",
      "\n",
      "Details: {'Brand': 'Contigo', 'Capacity': '1.5 Pounds', 'Color': 'Silver and Blue', 'Recommended Uses For Product': 'Home', 'Age Range (Description)': 'Adult', 'Product Dimensions': '2.75 x 2.75 x 11 inches', 'Model Name': 'Auto Seal Chill', 'Item Weight': '8.6 ounces', 'Theme': 'Sport', 'Material': 'Aluminum', 'Number of Items': '1', 'Included Components': 'Bottle^Lid', 'Product Care Instructions': 'Hand Wash Only', 'Cap Type': 'Loop Cap', 'Manufacturer': 'Contigo', 'Item model number': '2001714', 'Best Sellers Rank': {'Sports & Outdoors': 30233, 'Water Bottles': 801}, 'Is Discontinued By Manufacturer': 'No', 'Date First Available': 'December 24, 2016'}\n",
      "\n",
      "-----\n",
      "Rating: 5\n",
      "\n",
      "Title: Great water bottle\n",
      "\n",
      "Text: This turned out to be much better than I expected.  What I really like is that it is easy to drink out of but doesnt leak at all when I set it down on its side.<br /><br />In addition it has great insulation.  I leave this sitting in the sun with cold water in it all the time.  The bottle is hot to the touch in the outside.  But the water is still just as cold.<br /><br />I highly recommend it.\n",
      "\n",
      "-----\n",
      "Rating: 5\n",
      "\n",
      "Title: Keeps it cold for a very long time!\n",
      "\n",
      "Text: I drink more water when the water is cold so having it stay cold for a long time is perfect for me.  Also fits in cup holders so it is very convenient.\n",
      "\n",
      "-----\n",
      "Rating: 5\n",
      "\n",
      "Title: Favorite\n",
      "\n",
      "Text: Been using it everyday for a while now no leaks easy to use perfect size\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply random example generator for best products \n",
    "df = filtered_df\n",
    "column = 'parent_asin'\n",
    "interest = 'B08XXQSHH6'\n",
    "generate_random_examples(df, column, interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4ebc61-3d17-40b4-9f23-320f6244a422",
   "metadata": {},
   "source": [
    "## Preprocess text column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00670d05-10fc-4aa4-b936-e83f8ab48588",
   "metadata": {},
   "source": [
    "### 1. Remove the noise from the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d571c2f4-dd78-411f-9951-2aa4e44a2585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>5</td>\n",
       "      <td>Will get another.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>5</td>\n",
       "      <td>Doesn't leak which was the reason I bought it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                            text\n",
       "1326       5                               Will get another.\n",
       "137        5  Doesn't leak which was the reason I bought it."
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def regex_clean(text):\n",
    "    # convert html escapes like &amp; to characters.\n",
    "    text = html.unescape(text)\n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # Remove all punctuation except periods and apostrophes\n",
    "    text = re.sub(r\"[^\\w\\s'.]\", '', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "filtered_df['text'] = filtered_df['text'].apply(regex_clean)\n",
    "filtered_df[['rating', 'text']].sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2af0ba-e5e0-4949-8e59-709c313fa354",
   "metadata": {},
   "source": [
    "### 2. Lemmatize and tokenize the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ef9e8a1d-929f-4eff-829b-912b5eef1153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[VIDEOID:c87e962bc893a948856b0f1b285ce6cc]] I wanted to love this bc I previously bought a matching turquoise teapot, but the loose lid (defective or design flaw? Idk) on the cups is driving me batty. I’m disabled so my gait is not great to begin with & the lid just bangs non-stop while I walk from my kitchen to wherever I’m going with my tea. It’s incredibly annoying.  I had hoped it was just a one-off so I purchased it in another color & sadly it has the same exact problem.  They could fix the problem by adding a rubber gasket or flange to the lid imo & I even thought of doing so myself until I accidentally knocked the cup over due to a design flaw that has a small base on the cup.  I like the lid bc I run a fan continuously & I live with 2 service dogs so I like to keep my drinks covered beyond just the brew times so I really hope they update this cup bc it does keep the tea warm & the size is perfect for a 2 cup brew.<br /><br />I wish they would fix the obvious design flaw of the base that is too small for a cup that tall & wide bc it is also very easy to accidentally knock over as I mentioned above.  I did it several times before I yet boxed them both back up.<br /><br />Overall I like the product as it seems as it was made with quality in mind, but missed the mark in the design phase.  The colors are pretty & the cups I purchased keep the tea warm, but the lids are either defective or a design flaw.  I really hope the company fixes these issue bc the colors match teapots I already own. Lol.  I intend to see if the company has another design that works better for me in the meantime.\n"
     ]
    }
   ],
   "source": [
    "text = \"[[VIDEOID:c87e962bc893a948856b0f1b285ce6cc]] I wanted to love this bc I previously bought a matching turquoise teapot, but the loose lid (defective or design flaw? Idk) on the cups is driving me batty. I’m disabled so my gait is not great to begin with & the lid just bangs non-stop while I walk from my kitchen to wherever I’m going with my tea. It’s incredibly annoying.  I had hoped it was just a one-off so I purchased it in another color & sadly it has the same exact problem.  They could fix the problem by adding a rubber gasket or flange to the lid imo & I even thought of doing so myself until I accidentally knocked the cup over due to a design flaw that has a small base on the cup.  I like the lid bc I run a fan continuously & I live with 2 service dogs so I like to keep my drinks covered beyond just the brew times so I really hope they update this cup bc it does keep the tea warm & the size is perfect for a 2 cup brew.<br /><br />I wish they would fix the obvious design flaw of the base that is too small for a cup that tall & wide bc it is also very easy to accidentally knock over as I mentioned above.  I did it several times before I yet boxed them both back up.<br /><br />Overall I like the product as it seems as it was made with quality in mind, but missed the mark in the design phase.  The colors are pretty & the cups I purchased keep the tea warm, but the lids are either defective or a design flaw.  I really hope the company fixes these issue bc the colors match teapots I already own. Lol.  I intend to see if the company has another design that works better for me in the meantime.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6af64a3d-cfae-4073-8be8-d1cea1201e1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'matching turquoise teapot loose lid defective design flaw cup batty disabled gait great lid non - kitchen tea annoying off color same exact problem problem rubber gasket flange lid cup design flaw small base cup lid fan service dog drink brew time cup tea warm size perfect cup obvious design flaw base small cup tall wide easy several time product quality mind mark design phase color pretty cup tea warm lid defective design flaw company issue color teapot lol company design meantime'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_lemmas_with_pos(text, pos_to_keep=('ADJ', 'NOUN')):\n",
    "    \"\"\"Extract lemmas while keeping nouns and adjectives based on their position.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc if token.pos_ in pos_to_keep]\n",
    "    all_lemmas = \" \".join(lemmas)\n",
    "    return all_lemmas\n",
    "    \n",
    "# Extract lemmas with nouns and adjectives\n",
    "extract_lemmas_with_pos(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "53b5cec6-3793-405b-861a-be618a442446",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original stopwords count: 301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'videoid want love bc previously buy matching turquoise teapot loose lid defective design flaw idk cup drive batty disable gait not great begin lid bang non stop walk kitchen tea incredibly annoying hope off purchase color sadly same exact problem fix problem add rubber gasket flange lid imo think accidentally knock cup due design flaw small base cup like lid bc run fan continuously live service dog like drink cover brew time hope update cup bc tea warm size perfect cup wish fix obvious design flaw base small cup tall wide bc very easy accidentally knock mention several time box like product quality mind miss mark design phase color pretty cup purchase tea warm lid defective design flaw hope company fix issue bc color match teapot own lol intend company design work well meantime'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "include_stopwords = {'would', 'I'}\n",
    "stop_words |= include_stopwords\n",
    "print('Original stopwords count:', len(stopwords))\n",
    "\n",
    "def clean_data(doc):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses a text document using spaCy.\n",
    "    \n",
    "    This function takes a text document as input, converts it to lowercase, \n",
    "    lemmatizes the words, removes non-alphabetic characters, and filters out stopwords. \n",
    "    The resulting cleaned text is returned as a single string. \n",
    "    \"\"\"\n",
    "    doc = doc.lower()\n",
    "    doc = nlp(doc)\n",
    "    # Lemmatize words \n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Removing non-alphabetic characters and stopwords\n",
    "    tokens = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in stopwords]\n",
    "    cleaned_doc = \" \".join(tokens)\n",
    "    \n",
    "    return cleaned_doc\n",
    "    \n",
    "clean_data(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a3384495-0432-4bb4-85a9-903d628860f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>5</td>\n",
       "      <td>contigo good not leak replace hot cold beverag...</td>\n",
       "      <td>leak hot cold beverage holder brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>5</td>\n",
       "      <td>love easy bottle fill water buy mom specifical...</td>\n",
       "      <td>easy bottle fill water mom mother day mom coup...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                               text  \\\n",
       "2077       5  contigo good not leak replace hot cold beverag...   \n",
       "413        5  love easy bottle fill water buy mom specifical...   \n",
       "\n",
       "                                                 lemmas  \n",
       "2077                leak hot cold beverage holder brand  \n",
       "413   easy bottle fill water mom mother day mom coup...  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the text\n",
    "filtered_df['text'] = filtered_df['text'].apply(clean_data)\n",
    "# Extract lemmas \n",
    "filtered_df['lemmas'] = filtered_df['text'].apply(extract_lemmas_with_pos)\n",
    "filtered_df[['rating', 'text', 'lemmas']].sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edadab93-99a9-4a48-acee-2c3c40f06918",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using TF-IDF \n",
    "\n",
    "Use TF-IDF vectorizer to transform the text into vectors based on the frequency of words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "dc600520-ad78-4411-882e-1a8b3662c3f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp_application' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[228], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Combine preprocessed sentences back into the original review structure \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m combined_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_application\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_sentences\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m sentences: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences])\n\u001b[0;32m      3\u001b[0m nlp_application[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_preprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m combined_sentences\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp_application' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine preprocessed sentences back into the original review structure \n",
    "combined_sentences = nlp_application['preprocessed_sentences'].apply(lambda sentences: [' '.join(sentence) for sentence in sentences])\n",
    "nlp_application['combined_preprocessed'] = combined_sentences.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b5939-8204-4e71-a734-00671657da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvect = TfidfVectorizer()\n",
    "tfidf_text = tfidfvect.fit_transform(nlp_application['combined_preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1243e95-84d8-46b6-a397-39cb743963d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d24630-12d8-4126-b280-e27998d9b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20216cc5-3b4e-4ff1-8c99-2b44277c52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter to specify number of summary sentences \n",
    "num_summary_sentence = 10\n",
    "\n",
    "# Sum the TF-IDF values for each sentence\n",
    "sent_sum = tfidf_text.sum(axis = 1)\n",
    "important_sent = np.argsort(sent_sum, axis = 0)[::-1]\n",
    "\n",
    "# Print three most import sentences in the order they appear in the article \n",
    "print(\"Most Important Sentences Based on TF-IDF:\")\n",
    "for i in range(0, len(nlp_application['combined_preprocessed'])):\n",
    "    if i in important_sent[:num_summary_sentence]:\n",
    "        print(nlp_application['combined_preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79711aa4-486b-4d51-93cb-69408b5a9c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with_tfidf(doc, num_summary_sentence = num_summary_sentence):\n",
    "    \"\"\"\n",
    "    Apply the TF-IDF vectorization and then aggregate the value to a sentence level.\n",
    "    Generate a score for each sentence as a sum of the TF-IDF values for each word in that sentence. \n",
    "    A sentence with a high score contains many important words as compared to other sentences in the column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the text into sentences \n",
    "    sentences = []\n",
    "    for text in doc:\n",
    "        sentences.extend(sent_tokenize(text))\n",
    "\n",
    "    # Compute TF-IDF for the sentences \n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_text = tfidf.fit_transform(sentences)\n",
    "    \n",
    "    # Sort the sentences in descending order by the sum of TF-IDF values \n",
    "    sent_sum = tfidf_text.sum(axis = 1)\n",
    "    important_sent = np.argsort(sent_sum, axis = 0)[::-1].flatten()\n",
    "    \n",
    "    # Collect the most important sentences \n",
    "    summary_sentences = [sentences[i] for i in important_sent[:num_summary_sentence]]\n",
    "\n",
    "    return summary_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ef9a5-e6d9-4a8c-a76f-5b4bf9d749f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_summary_sentence = 10\n",
    "summarize_with_tfidf(filtered_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b21fda-cf7e-4206-8584-2d7144a339a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using LSA \n",
    "Latent semantic analysis (LSA) assumes that words that are close in meaning will occur in the same documents. Use package sumy to provide multiple summarization methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863bc1a6-d45b-4a4d-976e-2722783c5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = 'english'\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "parser = PlaintextParser.from_string(filtered_df['text'], Tokenizer(LANGUAGE))\n",
    "summarizer = LsaSummarizer(stemmer)\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "for sentence in summarizer(parser.document, num_summary_sentence):\n",
    "    print (str(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82cd8c6-daf0-4af5-ac9b-a43df5ebfb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sentence = tfidf_summary = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee1af4e-53b9-4243-aa2e-66207e67978f",
   "metadata": {},
   "source": [
    "## Extractive Summarization with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be8992-9b19-48a8-aaeb-fa22e8d8a6e1",
   "metadata": {},
   "source": [
    "### 1. Create target labels \n",
    "The target label defines whether a particular post should be included in the summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "dc662041-a7e3-41f7-bb4f-626d42b03c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of threads for Training:  140\n",
      "Number of threads for Testing:  35\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2)\n",
    "train_split, test_split = next(gss.split(filtered_df, groups=filtered_df['parent_asin']))\n",
    "train_df = filtered_df.iloc[train_split]\n",
    "test_df = filtered_df.iloc[test_split]\n",
    "print('Number of threads for Training: ', train_df['parent_asin'].nunique())\n",
    "print('Number of threads for Testing: ', test_df['parent_asin'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc62649-20c3-40b2-a204-1c53d1db1822",
   "metadata": {},
   "source": [
    "### Vectorize the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "4577eab1-974c-452b-ae86-d4bbc2d718fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data into numerical form using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(filtered_df['text'])\n",
    "\n",
    "\n",
    "# Convert the sparse matrix to a dense format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b07ba417",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse array length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[231], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfiltered_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtfidf_matrix\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(filtered_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4091\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4088\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4090\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4091\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4300\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4292\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4293\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4298\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4299\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4300\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4303\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4304\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4305\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4306\u001b[0m     ):\n\u001b[0;32m   4307\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5039\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5039\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:560\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequire_length_match\u001b[39m(data, index: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    557\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124;03m    Check the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:395\u001b[0m, in \u001b[0;36m_spbase.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 395\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse array length is ambiguous; use getnnz()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or shape[0]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: sparse array length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b8ec782-ca8b-4cae-9677-2da50fac1768",
   "metadata": {},
   "source": [
    "### Sentence Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abff31c-0bab-43b3-8ab8-7770431998ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences(filtered_df['text'], X):\n",
    "    sentence_scores = tfidf_matrix.sum(axis=1).A1\n",
    "    ranked_sentences = [sentences[i] for i in sentence_scores.argsort()[::-1]]\n",
    "    return ranked_sentences\n",
    "\n",
    "filtered_df['ranked_sentences'] = filtered_df.apply(lambda x: rank_sentences(x['sentences'], x['tfidf_matrix']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d629f-591c-4510-8cfd-b95e977a2033",
   "metadata": {},
   "source": [
    "### K-means clustering\n",
    "Use either methods to determine the optimal cluster: \n",
    "1. Elbow method:\n",
    "    - Involves plotting the within-cluster sum of squares (WCSS) against the number of clusters.\n",
    "    - The elbow point (where the WCSS starts to level off) shows the optimal number of clusters. Adding more clusters beyond that point doesn't significantly reduce WCSS.\n",
    "2. Silhouette method:\n",
    "    - Evaluates how well each point lies within its cluster.\n",
    "    - The optimal number of clusters is the one with the highest average silhouette score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a54a84-7a8b-4409-8459-7cc8c4694979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the elbow method to find the optimal number of clusters\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "# Plotting the elbow graph\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe30f8ca-ec9d-44f4-bc70-7ef5e1b2cbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(random_state=42)\n",
    "visualizer = KElbowVisualizer(km, k=(1,11))\n",
    " \n",
    "visualizer.fit(X_dense) \n",
    "visualizer.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a376d4-192d-4e6b-93e5-f72a2d69a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(random_state=42)\n",
    "visualizer = SilhouetteVisualizer(km, k=(1,11))\n",
    " \n",
    "visualizer.fit(X_dense) \n",
    "visualizer.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841376e8-43ef-438b-8165-ffaeb660cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8  # Choose based on the elbow plot\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "y_kmeans = kmeans.fit_predict(X_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d078b-08d7-4468-a04e-b489cb5353f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['cluster'] = y_kmeans\n",
    "print(filtered_df['cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708b79b-bf40-47b2-9ed2-a00a9c99e0c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    print(f\"Cluster {i}\")\n",
    "    print(filtered_df[filtered_df['cluster'] == i]['text'].head(5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656acea6-831a-4a17-9095-208a81407fb7",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
