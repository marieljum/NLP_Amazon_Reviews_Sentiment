{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf79bba4-138d-4937-810a-99ac2354ed4c",
   "metadata": {},
   "source": [
    "# Text Summarization of Amazon Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c66c1d2d-5190-4a5b-a4dd-08e620fd81f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import seaborn as sns\n",
    "import random\n",
    "import html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import textacy\n",
    "\n",
    "# Text cleaning\n",
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Cosine Similarity \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Text summarization \n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.utils import get_stop_words\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer \n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "import textdistance\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from nltk import sent_tokenize, word_tokenize, regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import gc\n",
    "import string \n",
    "import nltk \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad46dc4d-99a0-42a2-86e1-2be2a26e242b",
   "metadata": {},
   "source": [
    "## Load saved reviews and metadata csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b1d06e-3eae-42a0-9cba-3580294269e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rating', 'title', 'text', 'asin', 'parent_asin', 'user_id',\n",
       "       'timestamp', 'helpful_vote', 'verified_purchase', 'year',\n",
       "       'main_category', 'average_rating', 'rating_number', 'features',\n",
       "       'description', 'price', 'images', 'videos', 'store', 'categories',\n",
       "       'details', 'bought_together', 'subtitle', 'author'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df = pd.read_csv('../data/merge_df.csv', low_memory = False, index_col = False)\n",
    "merge_df = merge_df.drop(columns = {'Unnamed: 0'})\n",
    "merge_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502decd7-2ad8-45a4-8016-3c4451bbf0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null values in 'text' column \n",
    "merge_df = merge_df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4ebc61-3d17-40b4-9f23-320f6244a422",
   "metadata": {},
   "source": [
    "## Preprocess text column\n",
    "\n",
    "1. Remove the noise from the text data.\n",
    "2. Lemmatize and tokenize the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef9e8a1d-929f-4eff-829b-912b5eef1153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love my Autoseal Bottles. I do agree that they are &#34;sippy cups for grown ups&#34; but it prevents many MANY liquid spills. I drink a lot of water every day and this will keep my water cold for 8 hours easily - that includes putting ice cubes in the bottle at the start of the day - has even kept my cold water with ice cubes cold inside a car on an 80 degree Farenheit Los Angeles day.\n"
     ]
    }
   ],
   "source": [
    "text = \"I love my Autoseal Bottles. I do agree that they are &#34;sippy cups for grown ups&#34; but it prevents many MANY liquid spills. I drink a lot of water every day and this will keep my water cold for 8 hours easily - that includes putting ice cubes in the bottle at the start of the day - has even kept my cold water with ice cubes cold inside a car on an 80 degree Farenheit Los Angeles day.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aa8443-92f4-449a-a180-fdb5f03c7ffd",
   "metadata": {},
   "source": [
    "### 1. Remove noise with regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "25313b12-ca5e-4088-9e52-6483c60b0974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love my Autoseal Bottles. I do agree that they are sippy cups for grown ups but it prevents many MANY liquid spills. I drink a lot of water every day and this will keep my water cold for 8 hours easily that includes putting ice cubes in the bottle at the start of the day has even kept my cold water with ice cubes cold inside a car on an 80 degree Farenheit Los Angeles day.'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def regex_clean(text):\n",
    "    # convert html escapes like &amp; to characters.\n",
    "    text = html.unescape(text)\n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # Remove all punctuation except periods and apostrophes\n",
    "    text = re.sub(r\"[^\\w\\s'.]\", '', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "regex_clean(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a3384495-0432-4bb4-85a9-903d628860f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>357738</th>\n",
       "      <td>5</td>\n",
       "      <td>LOVE this product We keep it in the car for pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814286</th>\n",
       "      <td>5</td>\n",
       "      <td>My wife and I are HUGE Rangers fan this tree t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rating                                               text\n",
       "357738       5  LOVE this product We keep it in the car for pi...\n",
       "814286       5  My wife and I are HUGE Rangers fan this tree t..."
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the text\n",
    "merge_df['text'] = merge_df['text'].apply(regex_clean)\n",
    "merge_df[['rating', 'text']].sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92231a4-5660-443d-992e-a637b10a7778",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. Lemmatize and tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5cec6-3793-405b-861a-be618a442446",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "include_stopwords = {'would', 'I'}\n",
    "stopwords |= include_stopwords\n",
    "print('Original stopwords count:', len(stopwords))\n",
    "\n",
    "def clean_data(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses a text document using spaCy.\n",
    "    \n",
    "    This function takes a text document as input, converts it to lowercase, \n",
    "    lemmatizes the words, removes non-alphabetic characters, and filters out stopwords. \n",
    "    The resulting cleaned text is returned as a single string. \n",
    "    \"\"\"\n",
    "    #  convert to lowercase\n",
    "    text = text.lower()\n",
    "    # process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    # Lemmatize words \n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    tokens = [lemma for lemma in lemmas if lemma not in stopwords]\n",
    "    # # Removing non-alphabetic characters and stopwords\n",
    "    # tokens = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in stopwords]\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "    \n",
    "clean_data(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f28cf21-7258-487d-9d4e-fee1b7e7611b",
   "metadata": {},
   "source": [
    "## Feature Engingeering\n",
    "1. Identify positive and negative reviews based on rating.\n",
    "- Ratings of <= 4 are positive. Ratings of 3 => are negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "12921251-826f-4f26-b19a-d3f3cbd62f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df['positive_rating'] = 0\n",
    "\n",
    "# Classify records with rating higher than or equal to 4, positive (1)\n",
    "merge_df.loc[merge_df['rating'] >= 4, 'positive_rating'] = 1\n",
    "\n",
    "# Classify records with rating less than and equal to 3, negative (0)\n",
    "merge_df.loc[merge_df['rating'] < 4 , 'positive_rating'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "566a0f1f-44af-4264-8d9c-cacf7efb6cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>positive_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>5</td>\n",
       "      <td>I originally purchased a 3 pack of these and l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>2</td>\n",
       "      <td>Triangle screws Really Hubby undid the top thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                               text  \\\n",
       "1830       5  I originally purchased a 3 pack of these and l...   \n",
       "653        2  Triangle screws Really Hubby undid the top thi...   \n",
       "\n",
       "      positive_rating  \n",
       "1830                1  \n",
       "653                 0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df[['rating', 'text', 'positive_rating']].sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47834b08-0725-4661-bf11-44688225b575",
   "metadata": {},
   "source": [
    "## Filtering the dataframe to a specific brand or product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0555b2f7-f431-4a6d-9ab1-612f65d9491b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique store count: 245290\n",
      "-----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "store\n",
       "Coleman                   11886\n",
       "Fitbit                     6787\n",
       "CamelBak                   4897\n",
       "Alvada                     4339\n",
       "Franklin Sports            4142\n",
       "BalanceFrom                3563\n",
       "Amazon Basics              3516\n",
       "WILSON                     3178\n",
       "WinCraft                   3094\n",
       "CAP Barbell                3026\n",
       "Outdoorsman Lab            2971\n",
       "FOCO                       2910\n",
       "Schwinn                    2877\n",
       "Contigo                    2867\n",
       "Sunny Health & Fitness     2794\n",
       "SHIMANO                    2727\n",
       "adidas                     2611\n",
       "Gaiam                      2563\n",
       "Rico Industries            2452\n",
       "Nalgene                    2407\n",
       "Speedo                     2337\n",
       "Razor                      2304\n",
       "Sportneer                  2146\n",
       "Yes4All                    2103\n",
       "Northwest                  2095\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Unique store count: {merge_df['parent_asin'].nunique()}\")\n",
    "print('-----------')\n",
    "merge_df['store'].value_counts().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b16e2df2-d98d-4cef-a050-5f2e9e8ff4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rating', 'title', 'text', 'asin', 'parent_asin', 'user_id',\n",
       "       'timestamp', 'helpful_vote', 'verified_purchase', 'year',\n",
       "       'main_category', 'average_rating', 'rating_number', 'features',\n",
       "       'description', 'price', 'images', 'videos', 'store', 'categories',\n",
       "       'details', 'bought_together', 'subtitle', 'author'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning a specific store name\n",
    "store = 'Contigo'\n",
    "\n",
    "filtered_df = merge_df.loc[merge_df['store'] == store].copy()\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "filtered_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d35eabd-284a-41ca-877a-465e0a97f3ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store: Contigo\n",
      "-----\n",
      "Number of reviews in complete Amazon dataset: 192942\n",
      "Number of reviews in df: 2867\n",
      "Count of unique parent product IDs: 175\n",
      "Count of unique product ID's: 293\n",
      "-----\n",
      "Top ten products: \n",
      "     parent_asin  average_rating  rating_number  composite_score\n",
      "158  B08XXQSHH6             4.8        30463.0         146222.4\n",
      "169  B0BTHXPZWZ             4.8        12267.0          58881.6\n",
      "173  B0C4VWCZRV             4.5        12678.0          57051.0\n",
      "165  B0BT9JQ4LJ             4.7        10820.0          50854.0\n",
      "121  B07GBFYHNM             4.6         9365.0          43079.0\n",
      "164  B0BT9G6ZJC             4.7         7715.0          36260.5\n",
      "142  B07PDHRYSB             4.7         7354.0          34563.8\n",
      "162  B09R9X84HF             4.7         6619.0          31109.3\n",
      "65   B00YYBBYN8             4.6         5090.0          23414.0\n",
      "155  B08PWGXDFM             4.8         4754.0          22819.2\n",
      "-----\n",
      "Bottom ten products: \n",
      "     parent_asin  average_rating  rating_number  composite_score\n",
      "6    B003KZKD9K             3.5            2.0              7.0\n",
      "2    B00260GLOG             3.3            9.0             29.7\n",
      "119  B0786N8T17             3.9            8.0             31.2\n",
      "14   B004CYEB7S             4.0            8.0             32.0\n",
      "157  B08TX6H9SK             5.0            8.0             40.0\n",
      "5    B003KZKD7C             3.2           16.0             51.2\n",
      "12   B0044WIEAI             4.0           13.0             52.0\n",
      "3    B00260GLOQ             3.3           16.0             52.8\n",
      "0    B001QW0YP2             4.5           12.0             54.0\n",
      "28   B00A62JED6             4.1           14.0             57.4\n"
     ]
    }
   ],
   "source": [
    "def generate_brand_report(df, store):\n",
    "    \"\"\"\n",
    "    Generates a report for a specified store, including counts of unique parent and product IDs, \n",
    "    and statistics on reviews and ratings.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame by the specified store\n",
    "    filtered_df = df[df['store'] == store]\n",
    "    print(f\"Store: {store}\")\n",
    "    print('-----')\n",
    "\n",
    "    # Count number of reviews\n",
    "    complete_reviews_count = filtered_df['rating_number'].unique().sum()\n",
    "    print(f\"Number of reviews in complete Amazon dataset: {complete_reviews_count}\")\n",
    "    print(f\"Number of reviews in df: {len(filtered_df)}\")  # Some customers only include a title and rating to their reviews\n",
    "    \n",
    "    # Calculate the number of unique parent_asin values\n",
    "    parent_asin_count = filtered_df['parent_asin'].nunique()\n",
    "    print(f\"Count of unique parent product IDs: {parent_asin_count}\")\n",
    "    \n",
    "    # Count the number of unique asin values\n",
    "    asin_count = filtered_df['asin'].nunique()\n",
    "    print(f\"Count of unique product ID's: {asin_count}\")\n",
    "    \n",
    "    # Calculate the number of reviews and average review rating for each asin\n",
    "    asin_reviews_ratings = filtered_df.groupby('parent_asin').agg({'average_rating': 'mean', 'rating_number': 'mean'}).reset_index()\n",
    "\n",
    "    # Create a composite score for each product\n",
    "    asin_reviews_ratings['composite_score'] = asin_reviews_ratings['average_rating'] * asin_reviews_ratings['rating_number']\n",
    "\n",
    "    \n",
    "    # Find the top and bottom ten products with the best and worst rating\n",
    "    top_ten_best = asin_reviews_ratings.nlargest(10, 'composite_score')\n",
    "    bottom_ten_worst = asin_reviews_ratings.nsmallest(10, 'composite_score')\n",
    "    print('-----')\n",
    "    print(f\"Top ten products: \\n {top_ten_best}\")\n",
    "    print('-----')\n",
    "    print(f\"Bottom ten products: \\n {bottom_ten_worst}\")\n",
    "\n",
    "# Apply function\n",
    "generate_brand_report(filtered_df, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30aaff2b-fe99-43b9-8703-06f40e7b3fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_examples(df, column, interest):\n",
    "    \"\"\"\n",
    "    Generate three examples of random records with the specified interest. \n",
    "    \n",
    "    Parameters: \n",
    "    df: DataFrame where the search will occur. \n",
    "    column: Column name to search within. \n",
    "    interest: Value to search for in the specified column. \n",
    "    \"\"\"\n",
    "    # Print average rating for parent_asin \n",
    "    interest_average_rating = df[df[column] == interest]['average_rating'].unique()\n",
    "    print(f'Average rating: {interest_average_rating[0]}\\n')\n",
    "    print(f'Categories: {df[df[column] == interest][\"categories\"].iloc[0]}\\n')\n",
    "    print(f'Details: {df[df[column] == interest][\"details\"].iloc[0]}\\n')\n",
    "    \n",
    "    # Filter the Datadf[df[column] == interest]Frame based on the interest\n",
    "    filtered_df = df[df[column] == interest]\n",
    "\n",
    "    # Set number of examples \n",
    "    num_of_ex = 3 \n",
    "    \n",
    "    # Check if there are enough records to sample\n",
    "    if len(filtered_df) < num_of_ex:\n",
    "        print(f\"Not enough records found for interest '{interest}'. Found {len(filtered_df)} records.\")\n",
    "        return\n",
    "    \n",
    "    # Randomly select three records\n",
    "    random_examples = filtered_df.sample(n=num_of_ex)\n",
    "    \n",
    "    for i, row in random_examples.iterrows():\n",
    "        print('-----')\n",
    "        print(f'Rating: {row[\"rating\"]}\\n')\n",
    "        print(f'Title: {row[\"title\"]}\\n')\n",
    "        print(f'Text: {row[\"text\"]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "117e62b5-116e-4bb5-9e37-10c7366bf81f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating: 4.8\n",
      "\n",
      "Categories: ['Sports & Outdoors', 'Sports & Outdoor Recreation Accessories', 'Sports Water Bottles']\n",
      "\n",
      "Details: {'Brand': 'Contigo', 'Capacity': '1.5 Pounds', 'Color': 'Silver and Blue', 'Recommended Uses For Product': 'Home', 'Age Range (Description)': 'Adult', 'Product Dimensions': '2.75 x 2.75 x 11 inches', 'Model Name': 'Auto Seal Chill', 'Item Weight': '8.6 ounces', 'Theme': 'Sport', 'Material': 'Aluminum', 'Number of Items': '1', 'Included Components': 'Bottle^Lid', 'Product Care Instructions': 'Hand Wash Only', 'Cap Type': 'Loop Cap', 'Manufacturer': 'Contigo', 'Item model number': '2001714', 'Best Sellers Rank': {'Sports & Outdoors': 30233, 'Water Bottles': 801}, 'Is Discontinued By Manufacturer': 'No', 'Date First Available': 'December 24, 2016'}\n",
      "\n",
      "-----\n",
      "Rating: 5\n",
      "\n",
      "Title: Holds water\n",
      "\n",
      "Text: Great\n",
      "\n",
      "-----\n",
      "Rating: 5\n",
      "\n",
      "Title: Great water bottle\n",
      "\n",
      "Text: This keeps water cold most of the day.  Keeps ice very well.  I like the vacuum seal much like there coffee mugs.  This  is so nice as I have knocked it over on my desk a few times but it does not leak/spill.\n",
      "\n",
      "-----\n",
      "Rating: 5\n",
      "\n",
      "Title: Ice lasts forever\n",
      "\n",
      "Text: Bought this for work -- I drink a lot of water throughout the day and had been using a Camelback Podium Chill 24 oz. Now, that's a very different product designed for a different use case (cycling) and only has a thin layer of styrofoam insulation. Still, packing it full of ice and filling it with water, with refills of water throughout the day, I'd only get about 5 hours before the ice was all melted.<br /><br />First day with this 32 oz Contigo, I packed it full of ice (probably 20 oz worth) and topped it off with water, and refilled from the water cooler throughout the day. 10 hours later I still had probably 60-70% of the ice left in there! Filled it up again that night and the next morning it still had a decent amount of ice in it.<br /><br />Very convenient to drink from, the loop makes it easy to carry. Feels extremely well made and durable. The only (extremely minor) downsides are that it is too big to fit into my bottle side pockets on my backpack and that it can be a bit inconvenient to clean, like any water bottle with a body wider than its neck. Those are the most minor of considerations and it is easily a 5 star product for me.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply random example generator for best products \n",
    "df = filtered_df\n",
    "column = 'parent_asin'\n",
    "interest = 'B08XXQSHH6'\n",
    "generate_random_examples(df, column, interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491aeda-7f46-41d5-92bc-1f99f96b5fc5",
   "metadata": {},
   "source": [
    "### Filtering to a specific product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6bf688a7-e43a-457d-96fa-73fd73f41583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = 'B08XXQSHH6'\n",
    "\n",
    "filtered_df = filtered_df.loc[filtered_df['parent_asin'] == product]\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d310a7d1-eda5-4e7d-a426-2d544e881317",
   "metadata": {},
   "source": [
    "## Summarization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edadab93-99a9-4a48-acee-2c3c40f06918",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using TF-IDF \n",
    "\n",
    "Use TF-IDF vectorizer to transform the text into vectors based on the frequency of words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc600520-ad78-4411-882e-1a8b3662c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed sentences back into the original review structure \n",
    "combined_sentences = nlp_application['preprocessed_sentences'].apply(lambda sentences: [' '.join(sentence) for sentence in sentences])\n",
    "nlp_application['combined_preprocessed'] = combined_sentences.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b5939-8204-4e71-a734-00671657da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvect = TfidfVectorizer()\n",
    "tfidf_text = tfidfvect.fit_transform(nlp_application['combined_preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1243e95-84d8-46b6-a397-39cb743963d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d24630-12d8-4126-b280-e27998d9b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20216cc5-3b4e-4ff1-8c99-2b44277c52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter to specify number of summary sentences \n",
    "num_summary_sentence = 10\n",
    "\n",
    "# Sum the TF-IDF values for each sentence\n",
    "sent_sum = tfidf_text.sum(axis = 1)\n",
    "important_sent = np.argsort(sent_sum, axis = 0)[::-1]\n",
    "\n",
    "# Print three most import sentences in the order they appear in the article \n",
    "print(\"Most Important Sentences Based on TF-IDF:\")\n",
    "for i in range(0, len(nlp_application['combined_preprocessed'])):\n",
    "    if i in important_sent[:num_summary_sentence]:\n",
    "        print(nlp_application['combined_preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79711aa4-486b-4d51-93cb-69408b5a9c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with_tfidf(doc, num_summary_sentence = num_summary_sentence):\n",
    "    \"\"\"\n",
    "    Apply the TF-IDF vectorization and then aggregate the value to a sentence level.\n",
    "    Generate a score for each sentence as a sum of the TF-IDF values for each word in that sentence. \n",
    "    A sentence with a high score contains many important words as compared to other sentences in the column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the text into sentences \n",
    "    sentences = []\n",
    "    for text in doc:\n",
    "        sentences.extend(sent_tokenize(text))\n",
    "\n",
    "    # Compute TF-IDF for the sentences \n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_text = tfidf.fit_transform(sentences)\n",
    "    \n",
    "    # Sort the sentences in descending order by the sum of TF-IDF values \n",
    "    sent_sum = tfidf_text.sum(axis = 1)\n",
    "    important_sent = np.argsort(sent_sum, axis = 0)[::-1].flatten()\n",
    "    \n",
    "    # Collect the most important sentences \n",
    "    summary_sentences = [sentences[i] for i in important_sent[:num_summary_sentence]]\n",
    "\n",
    "    return summary_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ef9a5-e6d9-4a8c-a76f-5b4bf9d749f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_summary_sentence = 10\n",
    "summarize_with_tfidf(filtered_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b21fda-cf7e-4206-8584-2d7144a339a8",
   "metadata": {},
   "source": [
    "## Summarizing with Sumy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b9ead804-a182-4e1d-bafa-5640b41914c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of reviews \n",
    "reviews = filtered_df['text'].tolist()\n",
    "# Combine the list of reviews into one string\n",
    "combined_text = ' '.join(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e72f3f-73b2-413a-9923-14b4a74042ce",
   "metadata": {},
   "source": [
    "### LSA algorithm\n",
    "\n",
    "Latent semantic analysis (LSA) assumes that words that are close in meaning will occur in the same documents. Use package sumy to provide multiple summarization methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "410159db-34a2-41a1-b734-0e8f66ae0aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the most important feature of any travel bottlemug is a leak proof lid and on that front Contigo has nailed it again. We love these and have several Keep cool but leaks Handy way to get extra water into my body while I'm driving outside gardening at gkids sporting events. Since it seals itself automatically I dont have to worry about those colored sports drinks pouring out or leaking all over my car after the game. My only gripe is that the autoseal button had a minor cosmetic blemish so make sure you check your bottle upon receipt. It is perfectly designed and we love them I actually dumped most of our other water bottles out this week because these are so much better.\""
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LANGUAGE = 'english'\n",
    "\n",
    "def lsa_summarize(reviews, sentence_count=5):\n",
    "    \"\"\"\n",
    "    Summarizes the given string of reviews by extracting important sentences.\n",
    "    \"\"\"\n",
    "    parser = PlaintextParser.from_string(reviews, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = LsaSummarizer(stemmer)\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary = summarizer(parser.document, sentence_count)\n",
    "    \n",
    "    return ' '.join(str(sentence) for sentence in summary)\n",
    "\n",
    "lsa_summarize(combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6df65c-3fe3-408e-925b-229fbb430821",
   "metadata": {},
   "source": [
    "### Lex Rank\n",
    "\n",
    "A graphical based text summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9d95c678-3533-4029-8718-2d70ae371db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I like that they have a cap over the piece that you drink from but even when that is open it doesn't leak. I used to have a plastic contigo with the rubber spout you sucked from so I wasn't sure if I would like having to push a button and turn it upside down but its a great water bottle keeps my water cold for longer periods of time I can take it on the beach and it stays cold for a couple hours. Just an FYI in case you are interested the little leaflet that was inside the bottle when I got it which I actually didn't realize was in there and ended up washing along with the inside of the bottle before the first use....oops states the following paraphrased It is NOT for use with hot contents Do not use with carbonated or pulpy beverages Do not microwave duh...it's metal or freeze interesting If you wash the lid by hand you're supposed to soak it in hotsoapy water for 10 minutes Hand wash the body it should not be put in the dishwasher and you might want to get a bottle brush if you don't already have onethe body is very narrow and you probably won't be able to get your hand in it for thorough cleaning Probably shouldn't put the bottle in a bag or backpackthe autoseal button might accidentally get pushed and cause leakage I have been using a lot of glass reusable water bottles over the past couple years and I am looking forward to throwing this one into the rotation especially for bringing to the pool where glass water bottles aren't allowed and using near the treadmill the controlled water release makes it LOTS easier to drink out of while active as compared to a regular openmouthed bottle. It's a great bottle and I'd definitely recommend it Excellent manager of cold temperature and very easy to use Very nice I love this water bottle Several of my tennis cohorts use this same water bottle I finally had to get one after drinking from theirs way to often It is the Best Using for Water Contigo water bottles have changed my life. Love that you can use one hand to drink from this and it genuinely is leak proof Its the only water bottle I use Fits well in the car cut carrier too Contigo is the best Great Product.\""
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lexrank_summarize(reviews, sentence_count=5):\n",
    "    \"\"\"\n",
    "    Summarizes the given string of reviews by extracting important sentences.\n",
    "    \"\"\"\n",
    "    parser = PlaintextParser.from_string(reviews, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = LexRankSummarizer(stemmer)\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary = summarizer(parser.document, sentence_count)\n",
    "    \n",
    "    return ' '.join(str(sentence) for sentence in summary)\n",
    "\n",
    "lexrank_summarize(combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30bbe6-51a3-4712-89ab-f3d8f0f06ff2",
   "metadata": {},
   "source": [
    "### Luhn Summarizer\n",
    "\n",
    "It scores sentences based on frequency of the most important words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b00268f0-01ab-4594-9f86-10f715810233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I used to have a plastic contigo with the rubber spout you sucked from so I wasn't sure if I would like having to push a button and turn it upside down but its a great water bottle keeps my water cold for longer periods of time I can take it on the beach and it stays cold for a couple hours. Would definitely recommend to a friend and I have been singing its praises to all my family who are also hooked in Contigo products christmas gift looks great Great bottle Wonderful for the summer and winter so water doesn't freeze but stays a nice and cool temperature and for my husband he likes to keep the water room temperature no matter the weather outside so this works perfectly for him as well. Just an FYI in case you are interested the little leaflet that was inside the bottle when I got it which I actually didn't realize was in there and ended up washing along with the inside of the bottle before the first use....oops states the following paraphrased It is NOT for use with hot contents Do not use with carbonated or pulpy beverages Do not microwave duh...it's metal or freeze interesting If you wash the lid by hand you're supposed to soak it in hotsoapy water for 10 minutes Hand wash the body it should not be put in the dishwasher and you might want to get a bottle brush if you don't already have onethe body is very narrow and you probably won't be able to get your hand in it for thorough cleaning Probably shouldn't put the bottle in a bag or backpackthe autoseal button might accidentally get pushed and cause leakage I have been using a lot of glass reusable water bottles over the past couple years and I am looking forward to throwing this one into the rotation especially for bringing to the pool where glass water bottles aren't allowed and using near the treadmill the controlled water release makes it LOTS easier to drink out of while active as compared to a regular openmouthed bottle. It's a great bottle and I'd definitely recommend it Excellent manager of cold temperature and very easy to use Very nice I love this water bottle Several of my tennis cohorts use this same water bottle I finally had to get one after drinking from theirs way to often It is the Best Using for Water Contigo water bottles have changed my life. This water bottle is overall ok. Pros Opening is small enough that ice doesn't come through while drinking Great to have a water bottle that stays closed with the autoseal to prevent spills and prevent anything from getting into the bottle while being able to grab it with one hand to open it with the button and have a drink.\""
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def luhn_summarize(reviews, sentence_count=5):\n",
    "    \"\"\"\n",
    "    Summarizes the given string of reviews by extracting important sentences.\n",
    "    \"\"\"\n",
    "    parser = PlaintextParser.from_string(reviews, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = LuhnSummarizer(stemmer)\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary = summarizer(parser.document, sentence_count)\n",
    "    \n",
    "    return ' '.join(str(sentence) for sentence in summary)\n",
    "\n",
    "luhn_summarize(combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa7c907-6172-471c-a48e-e1cd5bea76db",
   "metadata": {},
   "source": [
    "### Text Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2aa79517-9953-41e8-8ffd-a2fe4fd5c3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I used to have a plastic contigo with the rubber spout you sucked from so I wasn't sure if I would like having to push a button and turn it upside down but its a great water bottle keeps my water cold for longer periods of time I can take it on the beach and it stays cold for a couple hours. Would definitely recommend to a friend and I have been singing its praises to all my family who are also hooked in Contigo products christmas gift looks great Great bottle Wonderful for the summer and winter so water doesn't freeze but stays a nice and cool temperature and for my husband he likes to keep the water room temperature no matter the weather outside so this works perfectly for him as well. Just an FYI in case you are interested the little leaflet that was inside the bottle when I got it which I actually didn't realize was in there and ended up washing along with the inside of the bottle before the first use....oops states the following paraphrased It is NOT for use with hot contents Do not use with carbonated or pulpy beverages Do not microwave duh...it's metal or freeze interesting If you wash the lid by hand you're supposed to soak it in hotsoapy water for 10 minutes Hand wash the body it should not be put in the dishwasher and you might want to get a bottle brush if you don't already have onethe body is very narrow and you probably won't be able to get your hand in it for thorough cleaning Probably shouldn't put the bottle in a bag or backpackthe autoseal button might accidentally get pushed and cause leakage I have been using a lot of glass reusable water bottles over the past couple years and I am looking forward to throwing this one into the rotation especially for bringing to the pool where glass water bottles aren't allowed and using near the treadmill the controlled water release makes it LOTS easier to drink out of while active as compared to a regular openmouthed bottle. It's a great bottle and I'd definitely recommend it Excellent manager of cold temperature and very easy to use Very nice I love this water bottle Several of my tennis cohorts use this same water bottle I finally had to get one after drinking from theirs way to often It is the Best Using for Water Contigo water bottles have changed my life. This water bottle is overall ok. Pros Opening is small enough that ice doesn't come through while drinking Great to have a water bottle that stays closed with the autoseal to prevent spills and prevent anything from getting into the bottle while being able to grab it with one hand to open it with the button and have a drink.\""
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def textrank_summarize(reviews, sentence_count=5):\n",
    "    \"\"\"\n",
    "    Summarizes the given string of reviews by extracting important sentences.\n",
    "    \"\"\"\n",
    "    parser = PlaintextParser.from_string(reviews, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = TextRankSummarizer(stemmer)\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary = summarizer(parser.document, sentence_count)\n",
    "    \n",
    "    return ' '.join(str(sentence) for sentence in summary)\n",
    "\n",
    "textrank_summarize(combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee1af4e-53b9-4243-aa2e-66207e67978f",
   "metadata": {},
   "source": [
    "## Extractive Summarization with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be8992-9b19-48a8-aaeb-fa22e8d8a6e1",
   "metadata": {},
   "source": [
    "### 1. Create target labels \n",
    "The target label defines whether a particular post should be included in the summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc662041-a7e3-41f7-bb4f-626d42b03c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2)\n",
    "train_split, test_split = next(gss.split(filtered_df, groups=filtered_df['parent_asin']))\n",
    "train_df = filtered_df.iloc[train_split]\n",
    "test_df = filtered_df.iloc[test_split]\n",
    "print('Number of threads for Training: ', train_df['parent_asin'].nunique())\n",
    "print('Number of threads for Testing: ', test_df['parent_asin'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc62649-20c3-40b2-a204-1c53d1db1822",
   "metadata": {},
   "source": [
    "### Vectorize the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577eab1-974c-452b-ae86-d4bbc2d718fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data into numerical form using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(filtered_df['text'])\n",
    "\n",
    "# Convert the sparse matrix to a dense format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ba417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b8ec782-ca8b-4cae-9677-2da50fac1768",
   "metadata": {},
   "source": [
    "### Sentence Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abff31c-0bab-43b3-8ab8-7770431998ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences(filtered_df['text'], X):\n",
    "    sentence_scores = tfidf_matrix.sum(axis=1).A1\n",
    "    ranked_sentences = [sentences[i] for i in sentence_scores.argsort()[::-1]]\n",
    "    return ranked_sentences\n",
    "\n",
    "filtered_df['ranked_sentences'] = filtered_df.apply(lambda x: rank_sentences(x['sentences'], x['tfidf_matrix']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d629f-591c-4510-8cfd-b95e977a2033",
   "metadata": {},
   "source": [
    "### K-means clustering\n",
    "Use either methods to determine the optimal cluster: \n",
    "1. Elbow method:\n",
    "    - Involves plotting the within-cluster sum of squares (WCSS) against the number of clusters.\n",
    "    - The elbow point (where the WCSS starts to level off) shows the optimal number of clusters. Adding more clusters beyond that point doesn't significantly reduce WCSS.\n",
    "2. Silhouette method:\n",
    "    - Evaluates how well each point lies within its cluster.\n",
    "    - The optimal number of clusters is the one with the highest average silhouette score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a54a84-7a8b-4409-8459-7cc8c4694979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the elbow method to find the optimal number of clusters\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "# Plotting the elbow graph\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe30f8ca-ec9d-44f4-bc70-7ef5e1b2cbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(random_state=42)\n",
    "visualizer = KElbowVisualizer(km, k=(1,11))\n",
    " \n",
    "visualizer.fit(X_dense) \n",
    "visualizer.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a376d4-192d-4e6b-93e5-f72a2d69a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(random_state=42)\n",
    "visualizer = SilhouetteVisualizer(km, k=(1,11))\n",
    " \n",
    "visualizer.fit(X_dense) \n",
    "visualizer.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841376e8-43ef-438b-8165-ffaeb660cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8  # Choose based on the elbow plot\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "y_kmeans = kmeans.fit_predict(X_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d078b-08d7-4468-a04e-b489cb5353f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['cluster'] = y_kmeans\n",
    "print(filtered_df['cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708b79b-bf40-47b2-9ed2-a00a9c99e0c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    print(f\"Cluster {i}\")\n",
    "    print(filtered_df[filtered_df['cluster'] == i]['text'].head(5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656acea6-831a-4a17-9095-208a81407fb7",
   "metadata": {},
   "source": [
    "## Extractive Summarization with Pre-trained Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a720f4d3-689e-432b-a6a6-6e4ab447c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(reviews):\n",
    "    \"\"\"\n",
    "    Splits the reviews into individual sentences.\n",
    "    \"\"\"\n",
    "    n_reviews = len(reviews)\n",
    "    for i in range(n_reviews):\n",
    "        review = reviews[i]\n",
    "        #print(email)\n",
    "        sentences = sent_tokenize(review)\n",
    "        #print(sentences)\n",
    "        for j in reversed(range(len(sentences))):\n",
    "            sent = sentences[j]\n",
    "            sentences[j] = sent.strip()\n",
    "            if sent == '':\n",
    "                sentences.pop(j)\n",
    "        reviews[i] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "546198f1-fc11-44b5-908a-16ba24d51e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_list = list(filtered_df['text'])\n",
    "split_sentences(rev_list)\n",
    "# Adding split reviews in the data frame\n",
    "filtered_df['sent_tokens'] = rev_list\n",
    "# Calculating lenght of sentences in each review\n",
    "filtered_df['length_of_rv'] = filtered_df['sent_tokens'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f337d644-ce0e-4a58-bbbb-ada7bfecc9f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "length_of_rv\n",
       "1     854\n",
       "2     575\n",
       "3     444\n",
       "4     308\n",
       "5     199\n",
       "6     132\n",
       "7      96\n",
       "8      57\n",
       "9      44\n",
       "10     31\n",
       "11     20\n",
       "13     16\n",
       "15     15\n",
       "12     14\n",
       "17     10\n",
       "14      9\n",
       "16      8\n",
       "18      6\n",
       "26      3\n",
       "28      3\n",
       "0       3\n",
       "19      3\n",
       "20      3\n",
       "21      3\n",
       "24      2\n",
       "30      2\n",
       "27      2\n",
       "22      1\n",
       "34      1\n",
       "23      1\n",
       "39      1\n",
       "36      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df['length_of_rv'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4f9d43e-f6b0-4658-b038-e1f9c3f1db8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>length_of_rv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>5</td>\n",
       "      <td>Really doesnt spill unless you flip it upside ...</td>\n",
       "      <td>[Really doesnt spill unless you flip it upside...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>4</td>\n",
       "      <td>Not bad just different than what were used to....</td>\n",
       "      <td>[Not bad just different than what were used to...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>5</td>\n",
       "      <td>Great for everyone. Kids can use them and they...</td>\n",
       "      <td>[Great for everyone., Kids can use them and th...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>4</td>\n",
       "      <td>Worked well but handle broke off after only ha...</td>\n",
       "      <td>[Worked well but handle broke off after only h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                               text  \\\n",
       "2640       5  Really doesnt spill unless you flip it upside ...   \n",
       "866        4  Not bad just different than what were used to....   \n",
       "1401       5  Great for everyone. Kids can use them and they...   \n",
       "1047       4  Worked well but handle broke off after only ha...   \n",
       "\n",
       "                                            sent_tokens  length_of_rv  \n",
       "2640  [Really doesnt spill unless you flip it upside...             2  \n",
       "866   [Not bad just different than what were used to...             4  \n",
       "1401  [Great for everyone., Kids can use them and th...             2  \n",
       "1047  [Worked well but handle broke off after only h...             1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df[['rating', 'text', 'sent_tokens', 'length_of_rv']].sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d5e28d8b-4a69-4b9b-a586-382c3290fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making vocabulary with reviews with max vocabs=5000. \n",
    "list_sentences_train = filtered_df['text']\n",
    "max_features = 5000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af9d37-5f69-4845-a3c2-11dd1d99ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbeddingMatrix(typeToLoad):\n",
    "        if(typeToLoad==\"glove\"):\n",
    "            EMBEDDING_FILE='glove.twitter.27B.25d.txt/glove.twitter.27B.25d.txt'\n",
    "            embed_size = 25\n",
    "        \n",
    "        elif(typeToLoad==\"fasttext\"):\n",
    "            EMBEDDING_FILE='wiki.simple.vec/wiki.simple.vec'\n",
    "            embed_size = 300\n",
    "\n",
    "        if(typeToLoad==\"glove\" or typeToLoad==\"fasttext\" ):\n",
    "            embeddings_index = dict()\n",
    "            #Transfer the embedding weights into a dictionary by iterating through every line of the file.\n",
    "            f = open(EMBEDDING_FILE, encoding='utf-8')\n",
    "            for line in f:\n",
    "                #split up line into an indexed array\n",
    "                values = line.split()\n",
    "                #first index is word\n",
    "                word = values[0]\n",
    "                #store the rest of the values in the array as a new array\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs #50 dimensions\n",
    "            f.close()\n",
    "            print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "       # else:\n",
    "        #    embeddings_index = dict()\n",
    "         #   for word in word2vecDict.wv.vocab:\n",
    "          #      embeddings_index[word] = word2vecDict.word_vec(word)\n",
    "           # print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "            \n",
    "        gc.collect()\n",
    "        return embeddings_index #, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15adee7b-2ec2-449b-91d4-8edb49ddbd76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summary\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Apply summarization to each review and store the summaries in a new column\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m filtered_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfiltered_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_summary\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[59], line 9\u001b[0m, in \u001b[0;36mgenerate_summary\u001b[1;34m(review_text)\u001b[0m\n\u001b[0;32m      6\u001b[0m sequence \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([review_text])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create a matrix of word embeddings for the review\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m review_embedding_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(sequence), \u001b[43membedding_dim\u001b[49m))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sequence):\n\u001b[0;32m     11\u001b[0m     review_embedding_matrix[i] \u001b[38;5;241m=\u001b[39m embedding_matrix_vocab\u001b[38;5;241m.\u001b[39mget(word_idx, np\u001b[38;5;241m.\u001b[39mzeros(embedding_dim))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_dim' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to generate summaries\n",
    "def generate_summary(review_text):\n",
    "    # Tokenize the review text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([review_text])\n",
    "    sequence = tokenizer.texts_to_sequences([review_text])[0]\n",
    "    \n",
    "    # Create a matrix of word embeddings for the review\n",
    "    review_embedding_matrix = np.zeros((len(sequence), embedding_dim))\n",
    "    for i, word_idx in enumerate(sequence):\n",
    "        review_embedding_matrix[i] = embedding_matrix_vocab.get(word_idx, np.zeros(embedding_dim))\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = cosine_similarity(review_embedding_matrix)\n",
    "    \n",
    "    # Calculate sentence scores based on similarity\n",
    "    sentence_scores = similarity_matrix.sum(axis=1)\n",
    "    \n",
    "    # Select top sentences for summary (e.g., top 3 sentences)\n",
    "    top_sentence_indices = sentence_scores.argsort()[-3:][::-1]\n",
    "    \n",
    "    # Construct summary from top sentences\n",
    "    summary = \"\"\n",
    "    for idx in top_sentence_indices:\n",
    "        summary += review_text.split('.')[idx] + \". \"\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "# Apply summarization to each review and store the summaries in a new column\n",
    "filtered_df['summary'] = filtered_df['text'].apply(generate_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
