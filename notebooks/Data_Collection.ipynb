{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1b0e46-7ab4-429c-bd4f-0b2682d41151",
   "metadata": {},
   "source": [
    "# Download Amazon Reviews Data\n",
    "\n",
    "This Amazon Reviews dataset is collected from the University of California's San Diego McAuley Lab (https://amazon-reviews-2023.github.io/main.html). \n",
    "\n",
    "It includes **user reviews, item metadata, and various product links.**\n",
    "\n",
    "The Amazon Reivews dataset has 34 categories. This project will focus on the Home and Kitchen cateogry, which has 23.2 million users, 3.7 million items, and 67.4 million ratings. \t\n",
    "\n",
    "### Citation \n",
    "    @article{hou2024bridging,\n",
    "      title={Bridging Language and Items for Retrieval and Recommendation},\n",
    "      author={Hou, Yupeng and Li, Jiacheng and He, Zhankui and Yan, An and Chen, Xiusi and McAuley, Julian},\n",
    "      journal={arXiv preprint arXiv:2403.03952},\n",
    "      year={2024}\n",
    "    }\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5310bbf8-b545-4fd0-88b5-ef0f930fcf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "from scipy.sparse import save_npz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61755914-9a6e-46b0-aa70-43aa955237a3",
   "metadata": {},
   "source": [
    "## Loading Reviews and Meta Data\n",
    "\n",
    "Read the JSON Lines file in chunks and combine the chunks into one dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a161181d-bac6-42c4-a780-398b1b6b8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = \"../data/Home_and_Kitchen.jsonl\"\n",
    "meta = \"../data/meta_Home_and_Kitchen.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0658e9a-dd0b-42e7-93b1-315ec9664aab",
   "metadata": {},
   "source": [
    "### Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ed96a6e-71da-4337-a1e6-882d1391373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_json_text(file, chunksize=1000000):\n",
    "    \"\"\"\n",
    "    Reads a JSON Lines file in chunks, fits the TF-IDF vectorizer on the first chunk,\n",
    "    and transforms subsequent chunks. The resulting sparse matrices are combined using vertical stacking.\n",
    "    \"\"\"\n",
    "\n",
    "    tfidf = TfidfVectorizer(stop_words = 'english', min_df = 2) \n",
    "    vectors = []  \n",
    "    first_chunk = True\n",
    "    total_rows = 0\n",
    "\n",
    "    # Read the JSON lines file in chunks\n",
    "    chunks = pd.read_json(file, lines=True, chunksize = chunksize)\n",
    "    n_chunks = 0\n",
    "    for chunk in chunks:            \n",
    "        chunk_df = pd.concat([chunk], ignore_index=True)\n",
    "        if first_chunk: \n",
    "            tfidf_matrix = tfidf.fit_transform(chunk_df['text'])\n",
    "            first_chunk = False\n",
    "        else: \n",
    "            tfidf_matrix = tfidf.transform(chunk_df['text'])\n",
    "            \n",
    "        vectors.append(tfidf_matrix)\n",
    "        n_chunks += 1  \n",
    "        print(f\"{len(chunk)} rows added\")\n",
    "        total_rows += len(chunk)\n",
    "\n",
    "        # if n_chunks >= 2:\n",
    "        #     break\n",
    "\n",
    "    print(\"Done\")\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    return vstack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27caed58-56f7-43a0-9950-8836e3486951",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "\n",
    "home_sparse_matrix = vectorize_json_text(reviews)\n",
    "\n",
    "end = time.process_time()\n",
    "elapsed_time = end - start\n",
    "print(f'Execution time: {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23a10e-5607-4a2c-8c22-cbecf9dba350",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8073a-9405-47b6-81f0-7cc3fd1e2bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reviews_chunks(file, chunksize=100000):\n",
    "    \"\"\"\n",
    "    Reads a JSON Lines file in chunks, removes the 'text' column,\n",
    "    and appends the chunk to a list of dataframes.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    total_rows = 0\n",
    "\n",
    "    # Read the JSON  file in chunks\n",
    "    chunks = pd.read_json(file, lines=True, chunksize = chunksize)\n",
    "    n_chunks = 0\n",
    "    \n",
    "    for chunk in chunks:    \n",
    "        chunk = chunk.drop(columns = 'text')\n",
    "        dfs.append(chunk)\n",
    "        n_chunks += 1  \n",
    "        total_rows += len(chunk)\n",
    "        print(f\"{len(chunk)} rows added\")        \n",
    "\n",
    "        # Uncomment the code below if you want a subset\n",
    "        if n_chunks >= 2:\n",
    "            break\n",
    "\n",
    "    print(\"Done\")\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e070fe6-917a-4665-9e91-29248115a84a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "\n",
    "home_reviews = process_reviews_chunks(reviews)\n",
    "\n",
    "end = time.process_time()\n",
    "elapsed_time = end - start\n",
    "print(f'Execution time: {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d413c-6986-4bb7-9c44-7ec4b1af4663",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757ed8e-700d-48fb-9e26-867f4d7b289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_meta_chunks(file, chunksize = 100000):\n",
    "    \"\"\"\n",
    "    Processes chunks of records from metadata JSON Lines file and appends them to a DataFrame.\n",
    "    \"\"\"\n",
    "    dfs = []  \n",
    "    total_rows = 0\n",
    "\n",
    "    # Read the JSON file in chunks\n",
    "    chunks = pd.read_json(file, lines=True, chunksize=chunksize, encoding='utf-8', encoding_errors='ignore')\n",
    "    n_chunks = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        dfs.append(chunk)\n",
    "        print(f\"{len(chunk)} rows added\")\n",
    "        n_chunks += 1 \n",
    "        total_rows += len(chunk)\n",
    "        # Uncomment the code below if you want to work with a subset\n",
    "        if n_chunks >= 2:  \n",
    "            break  \n",
    "            \n",
    "    print(\"Done\")\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20070a23-045b-4145-ad76-691b6f4a7e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "\n",
    "home_meta = process_meta_chunks(meta)\n",
    "\n",
    "end = time.process_time()\n",
    "elapsed_time = end - start\n",
    "print(f'Execution time: {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d9775-8fe6-4424-9b89-500e8457968f",
   "metadata": {},
   "source": [
    "## Export csv's and sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338bd3af-84b4-45ef-8f01-ab1fb5ca0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the reviews data sparse matrix \n",
    "# save_npz('../data/reviews_matrix.npz', home_sparse_matrix)\n",
    "\n",
    "# Export reviews and meta csv's \n",
    "# home_reviews = pd.read_csv('../data/home_reviews.csv')\n",
    "# home_meta = pd.read_csv('../data/home_metadata.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
