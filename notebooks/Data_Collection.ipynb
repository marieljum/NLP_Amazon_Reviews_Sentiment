{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1b0e46-7ab4-429c-bd4f-0b2682d41151",
   "metadata": {},
   "source": [
    "# Download Amazon Reviews Data\n",
    "\n",
    "This Amazon Reviews dataset is collected from the University of California's San Diego McAuley Lab (https://amazon-reviews-2023.github.io/main.html). \n",
    "\n",
    "It includes **user reviews, item metadata, and various product links.**\n",
    "\n",
    "The Amazon Reivews dataset has 34 categories. This project will focus on the Home and Kitchen cateogry, which has 23.2 million users, 3.7 million items, and 67.4 million ratings. \t\n",
    "\n",
    "### Citation \n",
    "    @article{hou2024bridging,\n",
    "      title={Bridging Language and Items for Retrieval and Recommendation},\n",
    "      author={Hou, Yupeng and Li, Jiacheng and He, Zhankui and Yan, An and Chen, Xiusi and McAuley, Julian},\n",
    "      journal={arXiv preprint arXiv:2403.03952},\n",
    "      year={2024}\n",
    "    }\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5310bbf8-b545-4fd0-88b5-ef0f930fcf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2edd66-c2a1-4a42-926d-928f4683a1b8",
   "metadata": {},
   "source": [
    "### Creating stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "580ccbf4-4a5f-4d65-a285-d55e8bc212d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original stopwords count: 301\n",
      "Updated stopwords count: 301\n",
      "------------\n",
      "set()\n",
      "Stopwords count: 301\n",
      "------------\n",
      "['is', 'â€˜ll', 'each', 'cannot', 'in', 'after', 'thru', 'some', 'why', 'say']\n"
     ]
    }
   ],
   "source": [
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(f'Original stopwords count: {len(stop_words)}')\n",
    "\n",
    "# Include/ exclude certain words\n",
    "exclude_stopwords = {'well', 'off', 'very', 'not', 'few', 'much'}\n",
    "stop_words -= exclude_stopwords\n",
    "print(f'Updated stopwords count: {len(stop_words)}')\n",
    "\n",
    "# Remove adjectives from stopwords list using spaCy\n",
    "exclude_adjectives = {word for word in stop_words if nlp(word)[0].pos_ == \"ADJ\"}\n",
    "print('------------')\n",
    "print(exclude_adjectives)\n",
    "stop_words -= exclude_adjectives\n",
    "print(f'Stopwords count: {len(stop_words)}')\n",
    "\n",
    "# Convert the final stop words set to a list\n",
    "print('------------')\n",
    "stop_words = list(stop_words)\n",
    "print(stop_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61755914-9a6e-46b0-aa70-43aa955237a3",
   "metadata": {},
   "source": [
    "## Load Reviews and Meta Data\n",
    "\n",
    "Read the JSON Lines file, vectorize its text content using TF-IDF, and combine the chunks into one dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a161181d-bac6-42c4-a780-398b1b6b8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = \"../data/Home_and_Kitchen.jsonl\"\n",
    "meta = \"../data/meta_Home_and_Kitchen.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0658e9a-dd0b-42e7-93b1-315ec9664aab",
   "metadata": {},
   "source": [
    "### Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6fbe6b1-5260-4b54-aac1-a943a1cca6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reviews_chunks(file, chunksize = 100000):\n",
    "    \"\"\"\n",
    "    This function reads a JSON Lines file in chunks, fits the TF-IDF vectorizer on the first chunk,\n",
    "    and transforms subsequent chunks. The resulting sparse matrices are combined using stacking.\n",
    "    \"\"\"\n",
    "\n",
    "    # Setting as global variables\n",
    "    global n, total_rows  \n",
    "    n = 1 \n",
    "    total_rows = 0\n",
    "\n",
    "    tfidf = TfidfVectorizer(stop_words = stop_words, min_df = 2)\n",
    "\n",
    "    # Read the file in chunks\n",
    "    chunks = pd.read_json(file, lines=True, chunksize=chunksize, encoding='utf-8', encoding_errors='ignore')\n",
    "    vectors = []  \n",
    "    first_chunk = True\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if first_chunk: \n",
    "            tfidf_matrix = tfidf.fit_transform(chunk['text'])\n",
    "            first_chunk = False\n",
    "        else: \n",
    "            tfidf_matrix = tfidf.transform(chunk['text'])\n",
    "        \n",
    "        vectors.append(tfidf_matrix)\n",
    "        print(f\"{len(chunk)} rows added\")\n",
    "        n += 1 \n",
    "        total_rows += len(chunk)\n",
    "            \n",
    "    print(\"Done\")\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    return vstack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e070fe6-917a-4665-9e91-29248115a84a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MJ\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n",
      "100000 rows added\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "\n",
    "home_reviews = process_reviews_chunks(reviews)\n",
    "\n",
    "end = time.process_time()\n",
    "print(end - start)\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d413c-6986-4bb7-9c44-7ec4b1af4663",
   "metadata": {},
   "source": [
    "### Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757ed8e-700d-48fb-9e26-867f4d7b289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_meta_chunks(file, chunksize = 100000):\n",
    "    \"\"\"\n",
    "    Processes chunks of records from metadata JSON Lines file and appends them to a DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Setting as global variables\n",
    "    global n, total_rows  \n",
    "    n = 1 \n",
    "    total_rows = 0\n",
    "\n",
    "    # Read the file in chunks\n",
    "    chunks = pd.read_json(file, lines=True, chunksize=chunksize, encoding='utf-8', encoding_errors='ignore')\n",
    "    dfs = []  \n",
    "    n_chunks = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        dfs.append(chunk)\n",
    "        print(f\"{len(chunk)} rows added\")\n",
    "        n += 1 \n",
    "        total_rows += len(chunk)\n",
    "        # Uncomment the code below if you want to work with a subset\n",
    "        # if n_chunks >= 10:  \n",
    "        #     break  \n",
    "            \n",
    "    print(\"Done\")\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20070a23-045b-4145-ad76-691b6f4a7e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "\n",
    "home_meta = process_meta_chunks(meta)\n",
    "\n",
    "end = time.process_time()\n",
    "print(end - start)\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d9775-8fe6-4424-9b89-500e8457968f",
   "metadata": {},
   "source": [
    "## Export csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338bd3af-84b4-45ef-8f01-ab1fb5ca0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_text = pd.read_csv('../data/cleaned_subset.csv')\n",
    "# home_meta = pd.read_csv('../data/home_meta.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
