{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1b0e46-7ab4-429c-bd4f-0b2682d41151",
   "metadata": {},
   "source": [
    "# Download Amazon Reviews Data\n",
    "\n",
    "This Amazon Reviews dataset is collected from the University of California's San Diego McAuley Lab (https://amazon-reviews-2023.github.io/main.html). \n",
    "\n",
    "It includes **user reviews, item metadata, and various product links.**\n",
    "\n",
    "The Amazon Reivews dataset has 34 categories. This project will focus on the Sports and Outdoors cateogry, which has 10.3 million users, 1.6 million items, and 19.6 million ratings. \t\n",
    "\n",
    "### Citation \n",
    "    @article{hou2024bridging,\n",
    "      title={Bridging Language and Items for Retrieval and Recommendation},\n",
    "      author={Hou, Yupeng and Li, Jiacheng and He, Zhankui and Yan, An and Chen, Xiusi and McAuley, Julian},\n",
    "      journal={arXiv preprint arXiv:2403.03952},\n",
    "      year={2024}\n",
    "    }\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310bbf8-b545-4fd0-88b5-ef0f930fcf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "from scipy.sparse import save_npz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61755914-9a6e-46b0-aa70-43aa955237a3",
   "metadata": {},
   "source": [
    "## Load Reviews and Meta Data\n",
    "\n",
    "Read the JSON Lines file in chunks and combine the chunks into one dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161181d-bac6-42c4-a780-398b1b6b8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = \"../data/Sports_and_Outdoors.jsonl\"\n",
    "meta = \"../data/meta_Sports_and_Outdoors.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0658e9a-dd0b-42e7-93b1-315ec9664aab",
   "metadata": {},
   "source": [
    "### Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8073a-9405-47b6-81f0-7cc3fd1e2bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reviews_chunks(file, chunksize=100000):\n",
    "    \"\"\"\n",
    "    Processes chunks of records from a JSON Lines file and appends them to a concatenated DataFrame.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    total_rows = 0\n",
    "    n_chunks = 0\n",
    "\n",
    "    # Read the JSON  file in chunks\n",
    "    chunks = pd.read_json(file, lines=True, chunksize = chunksize)  \n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Extract the product name from the 'details' columns\n",
    "        chunk['product_name'] = chunk['details'].apply(extract_product_name)\n",
    "\n",
    "        # Append the processed chunk to the dfs list\n",
    "        dfs.append(chunk)\n",
    "        n_chunks += 1  \n",
    "        total_rows += len(chunk)\n",
    "        print(f\"{len(chunk)} rows added\")        \n",
    "\n",
    "        # Uncomment the code below if you want a subset\n",
    "        # if n_chunks >= 2:  # 2 chunks \n",
    "        #     break\n",
    "\n",
    "    print(\"Done\")\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e070fe6-917a-4665-9e91-29248115a84a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "\n",
    "reviews = process_reviews_chunks(reviews)\n",
    "\n",
    "end = time.process_time()\n",
    "elapsed_time = end - start\n",
    "print(f'Execution time: {elapsed_time} seconds')\n",
    "print(f'Length of Reviews DataFrame: {len(reviews)}')\n",
    "print(reviews.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d413c-6986-4bb7-9c44-7ec4b1af4663",
   "metadata": {},
   "source": [
    "### Item Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757ed8e-700d-48fb-9e26-867f4d7b289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_meta_chunks(file, chunksize = 100000):\n",
    "    \"\"\"\n",
    "    Processes chunks of records from metadata JSON Lines file and appends them to a DataFrame.\n",
    "    \"\"\"\n",
    "    dfs = []  \n",
    "    total_rows = 0\n",
    "\n",
    "    # Read the JSON file in chunks\n",
    "    chunks = pd.read_json(file, lines=True, chunksize=chunksize, encoding='utf-8', encoding_errors='ignore')\n",
    "    n_chunks = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        dfs.append(chunk)\n",
    "        print(f\"{len(chunk)} rows added\")\n",
    "        n_chunks += 1 \n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "        # Uncomment the code below if you want to work with a subset\n",
    "        # if n_chunks >= 2:  \n",
    "        #     break  \n",
    "            \n",
    "    print(\"Done\")\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20070a23-045b-4145-ad76-691b6f4a7e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "\n",
    "meta = process_meta_chunks(meta)\n",
    "\n",
    "end = time.process_time()\n",
    "elapsed_time = end - start\n",
    "print(f'Execution time: {elapsed_time} seconds')\n",
    "print(f'Length of Meta DataFrame: {len(meta)}')\n",
    "print(meta.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da0a5d-1882-463f-9e93-60dfefcc152b",
   "metadata": {},
   "source": [
    "### Feature Engingeering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e85ad0-9a79-4259-be20-8a2eee6ff0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a year column from the timestamp column \n",
    "reviews['year'] = reviews['timestamp'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c6cbd-ee5e-4d12-b520-62d2744ebc5b",
   "metadata": {},
   "source": [
    "## Merge reviews and metadata dataframes and export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35ff8d-78b5-4d49-9a5c-9c12f2758883",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = pd.merge(reviews_subset, meta_subset, on = 'parent_asin')\n",
    "merge_df = merge_df.drop(columns = {'title_y', 'images_x', 'Unnamed: 0', \n",
    "                                   'bought_together', 'subtitle', 'author'}).rename(columns = {'title_x': 'title', 'images_y': 'images'})\n",
    "print(merge_df.info())\n",
    "print('---------------')\n",
    "\n",
    "# Export to csv \n",
    "merge_df.to_csv('../data/merge_df.csv')\n",
    "print('Exported merged DataFrame')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
