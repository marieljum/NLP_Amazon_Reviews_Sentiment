{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1b0e46-7ab4-429c-bd4f-0b2682d41151",
   "metadata": {},
   "source": [
    "# Download Amazon Reviews Data\n",
    "\n",
    "This Amazon Reviews dataset is collected from the University of California's San Diego McAuley Lab (https://amazon-reviews-2023.github.io/main.html). \n",
    "\n",
    "It includes **user reviews, item metadata, and various product links.**\n",
    "\n",
    "The Amazon Reivews dataset has 34 categories. This project will focus on the Home and Kitchen cateogry, which has 23.2 million users, 3.7 million items, and 67.4 million ratings. \t\n",
    "\n",
    "### Citation \n",
    "    @article{hou2024bridging,\n",
    "      title={Bridging Language and Items for Retrieval and Recommendation},\n",
    "      author={Hou, Yupeng and Li, Jiacheng and He, Zhankui and Yan, An and Chen, Xiusi and McAuley, Julian},\n",
    "      journal={arXiv preprint arXiv:2403.03952},\n",
    "      year={2024}\n",
    "    }\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5310bbf8-b545-4fd0-88b5-ef0f930fcf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "from scipy.sparse import save_npz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61755914-9a6e-46b0-aa70-43aa955237a3",
   "metadata": {},
   "source": [
    "## Loading Reviews and Meta Data\n",
    "\n",
    "Read the JSON Lines file in chunks and combine the chunks into one dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a161181d-bac6-42c4-a780-398b1b6b8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = \"../data/Home_and_Kitchen.jsonl\"\n",
    "meta = \"../data/meta_Home_and_Kitchen.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0658e9a-dd0b-42e7-93b1-315ec9664aab",
   "metadata": {},
   "source": [
    "### Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a5d9634-0017-4079-8245-c11697e7b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_json_text(file, chunksize=1000000):\n",
    "    \"\"\"\n",
    "    Reads a JSON Lines file in chunks, fits the TF-IDF vectorizer on the first chunk,\n",
    "    and transforms subsequent chunks. The resulting sparse matrices are combined using vertical stacking.\n",
    "    \"\"\"\n",
    "\n",
    "    tfidf = TfidfVectorizer(stop_words = 'english', min_df = 2) \n",
    "    vectors = []  \n",
    "    first_chunk = True\n",
    "    total_rows = 0\n",
    "    n_chunks = 0\n",
    "    container = []\n",
    "\n",
    "    # Read the JSON lines file manually to handle decoding string errors\n",
    "    with open(file, 'r', encoding = 'utf-8', errors = 'replace') as f: \n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except json.JSONDecodeError: \n",
    "                continue \n",
    "\n",
    "            # Check if the loaded data is a dictionary\n",
    "            if isinstance(data, dict):\n",
    "                container.append(data)\n",
    "            else:\n",
    "                print(\"Warning: Ignored line - Not a dictionary\")\n",
    "\n",
    "            # Check if container has reached chunksize\n",
    "            if len(container) >= chunksize: \n",
    "                chunk_df = pd.DataFrame(container)\n",
    "                if first_chunk: \n",
    "                    tfidf_matrix = tfidf.fit_transform(chunk_df['text'])\n",
    "                    first_chunk = False\n",
    "                else: \n",
    "                    tfidf_matrix = tfidf.transform(chunk_df['text'])                    \n",
    "                vectors.append(tfidf_matrix)\n",
    "                n_chunks += 1  \n",
    "                print(f\"{len(chunk_df)} rows added\")\n",
    "                total_rows += len(chunk_df)  \n",
    "                container = []\n",
    "\n",
    "                # Uncomment the code below if you want a subset\n",
    "                if n_chunks >= 2:\n",
    "                    break\n",
    "        \n",
    "        # Process any remaining lines \n",
    "        if container:\n",
    "            chunk_df = pd.DataFrame(container)\n",
    "            tfidf_matrix = tfidf.transform(chunk_df['text'])                    \n",
    "            vectors.append(tfidf_matrix)\n",
    "            print(f\"{len(chunk_df)} rows added\")\n",
    "            total_rows += len(chunk_df)   \n",
    "                        \n",
    "    print(\"Done\")\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    return vstack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27caed58-56f7-43a0-9950-8836e3486951",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m----> 3\u001b[0m home_sparse_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorize_json_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreviews\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[0;32m      6\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "Cell \u001b[1;32mIn[31], line 23\u001b[0m, in \u001b[0;36mvectorize_json_text\u001b[1;34m(file, chunksize)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Check if the loaded data is a dictionary\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mdata\u001b[49m, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m     24\u001b[0m     container\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "\n",
    "home_sparse_matrix = vectorize_json_text(reviews)\n",
    "\n",
    "end = time.process_time()\n",
    "elapsed_time = end - start\n",
    "print(f'Execution time: {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23a10e-5607-4a2c-8c22-cbecf9dba350",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8073a-9405-47b6-81f0-7cc3fd1e2bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reviews_chunks(file, chunksize=100000):\n",
    "    \"\"\"\n",
    "    Reads a JSON Lines file in chunks, removes the 'text' column,\n",
    "    and appends the chunk to a list of dataframes.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    total_rows = 0\n",
    "\n",
    "    # Read the JSON  file in chunks\n",
    "    chunks = pd.read_json(file, lines=True, chunksize = chunksize)\n",
    "    n_chunks = 0\n",
    "    \n",
    "    for chunk in chunks:    \n",
    "        chunk = chunk.drop(columns = 'text')\n",
    "        dfs.append(chunk)\n",
    "        n_chunks += 1  \n",
    "        total_rows += len(chunk)\n",
    "        print(f\"{len(chunk)} rows added\")        \n",
    "\n",
    "        # Uncomment the code below if you want a subset\n",
    "        if n_chunks >= 2:\n",
    "            break\n",
    "\n",
    "    print(\"Done\")\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e070fe6-917a-4665-9e91-29248115a84a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "\n",
    "home_reviews = process_reviews_chunks(reviews)\n",
    "\n",
    "end = time.process_time()\n",
    "elapsed_time = end - start\n",
    "print(f'Execution time: {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d413c-6986-4bb7-9c44-7ec4b1af4663",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757ed8e-700d-48fb-9e26-867f4d7b289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_meta_chunks(file, chunksize = 100000):\n",
    "    \"\"\"\n",
    "    Processes chunks of records from metadata JSON Lines file and appends them to a DataFrame.\n",
    "    \"\"\"\n",
    "    dfs = []  \n",
    "    total_rows = 0\n",
    "\n",
    "    # Read the JSON file in chunks\n",
    "    chunks = pd.read_json(file, lines=True, chunksize=chunksize, encoding='utf-8', encoding_errors='ignore')\n",
    "    n_chunks = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        dfs.append(chunk)\n",
    "        print(f\"{len(chunk)} rows added\")\n",
    "        n_chunks += 1 \n",
    "        total_rows += len(chunk)\n",
    "        # Uncomment the code below if you want to work with a subset\n",
    "        if n_chunks >= 2:  \n",
    "            break  \n",
    "            \n",
    "    print(\"Done\")\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20070a23-045b-4145-ad76-691b6f4a7e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "\n",
    "home_meta = process_meta_chunks(meta)\n",
    "\n",
    "end = time.process_time()\n",
    "elapsed_time = end - start\n",
    "print(f'Execution time: {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d9775-8fe6-4424-9b89-500e8457968f",
   "metadata": {},
   "source": [
    "## Export csv's and sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338bd3af-84b4-45ef-8f01-ab1fb5ca0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the reviews data sparse matrix \n",
    "save_npz('../data/reviews_matrix.npz', home_sparse_matrix)\n",
    "\n",
    "# Export reviews and meta csv's \n",
    "# home_reviews = pd.read_csv('../data/home_reviews.csv')\n",
    "# home_meta = pd.read_csv('../data/home_metadata.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
