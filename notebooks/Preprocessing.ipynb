{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1b0e46-7ab4-429c-bd4f-0b2682d41151",
   "metadata": {},
   "source": [
    "# Cleaning Text\n",
    "\n",
    "An important task in Natural Language Processing (NLP) involves text data cleaning. To optimize results, it's important to convert your text to its essential root words within the corpus while removing irrelevant symbols. \n",
    "\n",
    "- Converting words into lowercase\n",
    "- Removing leading and trailing whitespace\n",
    "- Removing punctuation\n",
    "- Removing stopwords\n",
    "- Removing special characters (numbers, emojis, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5310bbf8-b545-4fd0-88b5-ef0f930fcf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize, regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1a46bc-13af-4dbd-ac3e-0ce8ba2b8b5b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63853345-0daa-4ec4-b583-61a699b4e269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MJ\\AppData\\Local\\Temp\\ipykernel_11240\\1899408169.py:1: DtypeWarning: Columns (16,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merge_df = pd.read_csv('../data/merge_df2.csv', low_memory = True, index_col = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 2556296\n"
     ]
    }
   ],
   "source": [
    "merge_df = pd.read_csv('../data/merge_df2.csv', low_memory = True, index_col = False)\n",
    "# Remove null values in 'text' column \n",
    "merge_df = merge_df.dropna(subset=['text'])\n",
    "# Remove rows where 'text' column contains empty strings\n",
    "merge_df = merge_df[merge_df['text'].str.strip() != '']\n",
    "# Reset the index of the DataFrame\n",
    "merge_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f'Number of records: {len(merge_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83485280-fece-4366-af5e-c7f26389ec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 5\n",
      "\n",
      "Text: Just follow directions. Easy to get onto bike chain without actual tools. Nice.\n",
      "\n",
      "----------\n",
      "Rating: 4\n",
      "\n",
      "Text: I bought this shower tent to inspire my 13-year old to read in a private space.  The door and “window” zipper really work well and add a lot of privacy.  I am really surprised by how much my kid loves it.  The tent is pretty spacious and can fit a beanbag chair along with some books.  I recommend it just for the novelty of it.  ;)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_row = merge_df.sample(2)\n",
    "print(f'Rating: {random_row[\"rating\"].iloc[0]}\\n')\n",
    "print(f'Text: {random_row[\"text\"].iloc[0]}\\n')\n",
    "# print(f'Tokenized Sentences: {random_row[\"tokenized_sentences\"].iloc[0]}\\n')\n",
    "print('----------')\n",
    "print(f'Rating: {random_row[\"rating\"].iloc[1]}\\n')\n",
    "print(f'Text: {random_row[\"text\"].iloc[1]}\\n')\n",
    "# print(f'Tokenized Sentences: {random_row[\"tokenized_sentences\"].iloc[1]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e921a398-05c3-4f8c-b75d-e2445a44d4d9",
   "metadata": {},
   "source": [
    "## Text Cleaning 1: Remove noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cb4f6b4-c575-427b-91d3-b8a5f7959bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count: 2556296\n"
     ]
    }
   ],
   "source": [
    "def regex_clean(text):\n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # Remove all punctuation except periods and apostrophes\n",
    "    text = re.sub(r\"[^\\w\\s'.!]\", '', text)\n",
    "    # Replace any sequence of periods longer than one with a single period\n",
    "    text = re.sub(r'\\.{2,}', '. ', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Clean the text\n",
    "merge_df['text'] = merge_df['text'].apply(regex_clean)\n",
    "print(f'Record count: {len(merge_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72c565-6117-4f8b-bb0a-8690c66cdfbc",
   "metadata": {},
   "source": [
    "## Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "271081e3-207b-4a87-a457-f6ed6643303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences and create a new column\n",
    "merge_df['tokenized_sentences'] = merge_df['text'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d354161-ae41-4880-90d5-0b61c20db3a2",
   "metadata": {},
   "source": [
    "## Text Cleaning 2: Remove stopwords and lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8df44-cbf3-4474-b388-07a4d1fe95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "include_stopwords = {'would', 'I'}\n",
    "stopwords |= include_stopwords\n",
    "print('Original stopwords count:', len(stopwords))\n",
    "\n",
    "def clean_data(tokenized_sentences):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses tokenized sentences using spaCy.\n",
    "    \n",
    "    This function takes a list of tokenized sentences as input, converts each sentence to lowercase, \n",
    "    lemmatizes the words, and filters out stopwords. \n",
    "    The resulting cleaned tokenized sentences are returned as a list of lists. \n",
    "    \"\"\"\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        # Convert each tokenized sentence to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        # Process the tokenized sentence with spaCy\n",
    "        doc = nlp(sentence)\n",
    "        # Lemmatize words and filter out stopwords\n",
    "        tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "        cleaned_text = \" \".join(tokens)        \n",
    "        cleaned_sentences.append(cleaned_text)\n",
    "    \n",
    "    return cleaned_sentences\n",
    "\n",
    "# Apply the clean_data function to the tokenized_sentences column\n",
    "merge_df['clean_tokenized_sentences'] = merge_df['tokenized_sentences'].apply(clean_data)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df89dc2-e6a4-4c48-9ee3-365ff74fc6d6",
   "metadata": {},
   "source": [
    "## Remove records with null values and empty strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c31380c-4d5e-4d51-ae98-65bd01112e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 2553768\n"
     ]
    }
   ],
   "source": [
    "# Remove null values in 'text' column \n",
    "merge_df = merge_df.dropna(subset=['text'])\n",
    "# Remove rows where 'text' column contains empty strings\n",
    "merge_df = merge_df[merge_df['text'].str.strip() != '']\n",
    "# Reset the index of the DataFrame\n",
    "merge_df.reset_index(drop=True, inplace=True)\n",
    "print(f'Number of records: {len(merge_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd1620-fa81-4592-80e7-2a721c6809f7",
   "metadata": {},
   "source": [
    "## Feature Engingeering\n",
    "1. Identify positive and negative reviews based on rating. \n",
    "    - Ratings 4 or greater are positive.\n",
    "    - Ratings less than 4 are negative.\n",
    "2. Count how many sentences a review has. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b7fbe42-4844-4f4d-a6ad-be1dfb5b3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df['positive_rating'] = 0\n",
    "\n",
    "# Classify records with rating higher than or equal to 4, positive (1)\n",
    "merge_df.loc[merge_df['rating'] >= 4, 'positive_rating'] = 1\n",
    "\n",
    "# Classify records with rating less than and equal to 3, negative (0)\n",
    "merge_df.loc[merge_df['rating'] < 4 , 'positive_rating'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7f1ab16-c73c-46b6-b8c4-9165d7f72e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of sentences in each tokenized sentence list\n",
    "merge_df['sentence_count'] = merge_df['tokenized_sentences'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8e6c9ec-47ad-4697-aa6a-4664c0d78847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of words in each sentence \n",
    "def count_words(sentence):\n",
    "    return len(sentence.split())\n",
    "\n",
    "# Apply the function to each tokenized sentence list\n",
    "merge_df['word_count_per_sent'] = merge_df['tokenized_sentences'].apply(lambda x: [count_words(sentence) for sentence in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b6e956a-c73c-4a7f-9d9e-e0cd2196c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = merge_df.drop(columns = {'Unnamed: 0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f8d516-f496-445c-b4b4-3eb7bdb55f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word_count(text):\n",
    "#     \"\"\"\n",
    "#     Counts the number of words in the text.\n",
    "#     \"\"\"\n",
    "#     words = text.split()\n",
    "#     return len(words)\n",
    "\n",
    "\n",
    "# def avg_word_length(text):\n",
    "#     \"\"\"\n",
    "#     Returns the average word length in the text. \n",
    "#     \"\"\"\n",
    "#     # Check for empty or white-space only string \n",
    "#     if not text.strip():\n",
    "#         return 0\n",
    "        \n",
    "#     words = text.split()\n",
    "#     if not words:  # Check if words list is empty\n",
    "#         return 0\n",
    "        \n",
    "#     word_lengths = [len(word) for word in words]\n",
    "#     avg_word_length = sum(word_lengths)/len(words)\n",
    "    \n",
    "#     return(avg_word_length) \n",
    "\n",
    "\n",
    "# def exclamation_count(text):\n",
    "#     \"\"\"\n",
    "#     Returns the number of exclamations in the text.\n",
    "#     \"\"\"\n",
    "#     doc = nlp(text)\n",
    "#     exclamations = []\n",
    "#     for token in doc: \n",
    "#         if token.text == '!':\n",
    "#             exclamations.append(token.text)\n",
    "#     return len(exclamations)\n",
    "\n",
    "# merge_df['word_count'] = merge_df['text'].apply(word_count)\n",
    "# merge_df['avg_word_length'] = merge_df['text'].apply(avg_word_length)\n",
    "# merge_df['exclamation_count'] = merge_df['text'].apply(exclamation_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce93e3-3bdf-47b2-9ba6-ba6c52b8f75e",
   "metadata": {},
   "source": [
    "## Export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae4cb5a5-482b-4100-a4b7-8947d317f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_df.to_csv('../data/merge_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef79d8a0-3b7e-4f1c-8f71-903ec26ebd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export select columns of merge_df to save space and time when loading data in\n",
    "# merge_df_select = merge_df[['rating', 'text', 'asin', 'parent_asin', 'year', 'average_rating', 'rating_number', 'price', 'store',\n",
    "# 'details', 'tokenized_sentences', 'positive_rating', 'sentence_count', 'word_count_per_sent']]\n",
    "# merge_df_select.to_csv('../data/merge_df_select.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
